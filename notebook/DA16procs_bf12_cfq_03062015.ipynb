{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide code cells\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tools__\n",
    "\n",
    "For disk bandwidth test, we use Fio. Fio is short for Flexible IO, a versatile IO workload generator. It was written to benchmark or verify changes to the Linux IO subsystem by Linux developers. Fio is flexible enough to allow detailed workload setups, and it contains the necessary reporting to make sense of the data at completion. Fio is widely used as an industry standard benchmark, stress testing tool, and for IO verification purposes. Fio isn't tied to any OS. It works on any platform from Windows to HP-UX to Android. There are also native IO engines on Linux, Windows, Solaris, etc. This is a key feature to help many kinds of measurement repeatable across platforms.\n",
    "\n",
    "Fio also supports three different types of output formats. The “classic” output is the default which dumps workload statistics at the end of the workload. There's also support for a CSV format called Terse, though that's slowly diminishing in favor of a JSON-based output format. The latter is far more flexible and has the advantage of being simple to parse for people and computers. However, JSON format only available in later version of FIO. We prefer the terse format since it's available to more versions of FIO. Especially, it is an advantage on some of the hosts where users are not able to upgrade FIO version.\n",
    "\n",
    "__Test Design__\n",
    "\n",
    "We run FIO on the physical host with different number of FIO threads (from 1 to 16 concurrent threads). Then with the same configuration, we run it with on 5 different VM flavors (VM with one, two, four, eight, and sixteen cores). We also make the comparison with the case that each VM has only one thread but we use many concurrent active VMs on the same host to perform I/O operation. We use identical Operating Systems and identical OS' setting across platforms to make sure that the measurement result is reliable. The same fixed amount of data was used throughout the test. When there are many concurrent threads or concurrent VMs running in one test, we distribute the input data equally to all of them.\n",
    "\n",
    "With the described scenarios above, we group our tests into three screnarios and name them:\n",
    "\n",
    "* Bare-metal (BM): multiple threads concurrently read data from or write data to the disk. Data were read from or written to a directory placed on the same hard drive.\n",
    "* Single VM with multiple threads (SVM-MT): 5 different VM flavors (each flavor has different number of core) ran the same test with the involvement of 1 to 16 threads.\n",
    "* Multiple VMs with a single threads (MVM-ST): 16 VMs, each VM initiates only one process to read/write data. Each VM has one core to undersubscribe the number of core available on our system. We would like to prevent the CPU becoming the bottle neck.\n",
    "\n",
    "FIO runs a job after read the setting for the jobs from a configuration file. Our sample configuration file was follow the this sample form:\n",
    "\n",
    "    ; sample setting of one thread\n",
    "    [sequential-read]                      ; name of the thread\n",
    "    rw=read                                ; access type: sequential read\n",
    "    size=(4gb/number_of_parallel_threads)  ; amount to be read/written for one thread\n",
    "    directory=/home/cloud-user/fiodata     ; directory to read/write\n",
    "    ioengine=libaio                        ; I/O engine\n",
    "    direct=1                               ; disable OS buffering\n",
    "    iodepth=32                             ; max number of IO requests in the queue for the thread\n",
    "\n",
    "Where:\n",
    "* ___rw___ defines the kind of disk access pattern: read (sequential reads), write (sequential writes), randread (random reads), randwrite (random writes), or mixed. We specially focus only on the two basic access patterns which was happen mostly in the context of Big Data world: sequential read and sequential write. A typical example is the Hadoop File System (HDFS) which has only sequential read and sequential write.\n",
    "* ___size___ is the total size of input data for this job. Fio will run until this many bytes of data has been transferred. We choose the total transfer volume to be 4 GBs. This is large enough to avoid many kinds of buffering from the application layer to the OS layer and then to the hardware layer. In facts, some of our initial tests have showed that the smaller of size of transfered data can cause very ridiculous results because of the RAID controller's cache and disk's cache. For example, on some tests we have repoted aggregated throughput of 300MB/s on a single \"Seagate Constellation.2 1TB\" hard drive while one of professional harddrive benchmarking website (and also the manufacturer) claims that the maximum achivable throughput is just about 115 MB/s (http://www.storagereview.com/seagate_constellation2_and_constellation_es2_hard_drive_review). It turned out that the problem was from the RAID controller's setting. We enable write-back cache, so the controller immediate returns after the data was written to cache but not the disk.\n",
    "The data will be divided to threads equally between parallel threads:\n",
    "    + 1 thread: transfers entire 4 GBs\n",
    "    + 2 threads: each thread transfers 2 GBs\n",
    "    + 4 threads: each thread transfers 1 GBs\n",
    "    + 8 threads: each thread transfers 512 MBs\n",
    "    + 16 threads: each thread transfers 256 MBs\n",
    "* ___direct I/O___ set the value of O_DIRECT in the Linux kernel. I/O operations performed against files opened with O_DIRECT bypass the kernel's page cache, writing directly to the storage. Recall that the storage may itself store the data in a write-back cache (in our case, the storage has 64 MBs of write-back cache). There is no strict guarantee that the function will return only after all data has been transferred.\n",
    "* ___iodepth___ defines the I/O depth of the system. This defines how many io units to keep in flight against the file. The default is 1 for each file defined in the job discription. This value can be overridden with a larger value for higher concurrency. We use I/O depth of 32. In fact, the default I/O depth in some of the Linux distributions can even be set upto 256 or 512.\n",
    "* ___ioengine___ is libaio which indicates the application to use AIO (Asynchronous Linux I/O) subsystem. AIO help to overlaps processing with I/O Operations. Application can submit (batch of) I/O operation without waiting for completion. This benefits physical I/O in particular when prefetching data. Besides, it allows to accumulate read or write requests so that the I/O subsystem can optimize performance by grouping or reordering the IO requests in favor of sequential access and large request. It helps to separate calls for submission and completion indication and pipelines operations to improve throughput. It also helps to improved utilization of CPU and devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d61f416c6d83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import library to use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Set figure to display inside notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python34\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2305\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2306\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2307\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2309\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python34\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2226\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2227\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2228\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2229\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python34\\lib\\site-packages\\IPython\\core\\magics\\pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n",
      "\u001b[1;32mC:\\Python34\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python34\\lib\\site-packages\\IPython\\core\\magics\\pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \"\"\"\n\u001b[0;32m     87\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_argstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python34\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[1;34m(self, gui)\u001b[0m\n\u001b[0;32m   3087\u001b[0m         \"\"\"\n\u001b[0;32m   3088\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpylabtools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3089\u001b[1;33m         \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3091\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python34\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36mfind_gui_and_backend\u001b[1;34m(gui, gui_select)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \"\"\"\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Import library to use\n",
    "# Set figure to display inside notebook\n",
    "%matplotlib inline\n",
    "#### \n",
    "import numpy as np\n",
    "from pandas import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import *\n",
    "import matplotlib.pylab as pylab\n",
    "import scipy.stats as td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-71a4c6c3541a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdtresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/bw16p4gbf12cache_cfq.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdtdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdtindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdtdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'T[0-2][0-9]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "dtresult = pd.read_csv('data/bw16p4gbf12cache_cfq.csv', delimiter=' ', header=None)\n",
    "dtdata = dtresult[range(1,17)]\n",
    "dtindex = dtresult[0]\n",
    "dtdata.index = dtindex.str.replace('T[0-2][0-9]', '',case=False)\n",
    "\n",
    "# To calculate the average throughput, we first calculate average throughput each thread has in one test, then again take the\n",
    "# average of 5 tests\n",
    "avgtpin1test = dtdata.T.mean() # Average throughput of each thread/VM in one test\n",
    "avgtpin5tests = avgtpin1test.groupby(level=0).mean() # Average throughput of each thread/VM in 5 tests\n",
    "avgtpin5tests = avgtpin5tests / 1024 # Converting to Megabytes/second\n",
    "\n",
    "# Putting average throughput per thread/VM to vector before plotting\n",
    "bwsrbf = avgtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "bwswbf = avgtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "bwsrsvm1c = avgtpin5tests.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "bwswsvm1c = avgtpin5tests.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "bwsrsvm2c = avgtpin5tests.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "bwswsvm2c = avgtpin5tests.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "bwsrsvm4c = avgtpin5tests.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "bwswsvm4c = avgtpin5tests.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "bwsrsvm8c = avgtpin5tests.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "bwswsvm8c = avgtpin5tests.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "bwsrsvm16c = avgtpin5tests.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "bwswsvm16c = avgtpin5tests.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "\n",
    "bwsrmvm = avgtpin5tests.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "bwswmvm = avgtpin5tests.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n",
    "\n",
    "# Calculate aggregated throughput on each host of each test, then taking the average\n",
    "aggtpin1test = dtdata.T.sum() # Aggregate throughput on each host in one test\n",
    "avgaggtpintestgrouped = aggtpin1test.groupby(level=0)\n",
    "avgaggtpin5tests = avgaggtpintestgrouped.mean() # Then calculate the average of the total throughput through 5 tests\n",
    "avgaggtpin5tests = avgaggtpin5tests / 1024 # Convert to MB/s\n",
    "ciaggtpin5tests = avgaggtpintestgrouped.aggregate(lambda x: np.std(x) / np.sqrt(x.count()) * 1.96) # Confident interval of 95%\n",
    "ciaggtpin5tests = ciaggtpin5tests / 1024 # Convert to same scale with mean\n",
    "\n",
    "# Putting average aggregate throughput to vector before plotting\n",
    "aggbwsrbf = avgaggtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "aggbwswbf = avgaggtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "aggbwsrsvm1c = avgaggtpin5tests.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "aggbwswsvm1c = avgaggtpin5tests.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "aggbwsrsvm2c = avgaggtpin5tests.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "aggbwswsvm2c = avgaggtpin5tests.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "aggbwsrsvm4c = avgaggtpin5tests.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "aggbwswsvm4c = avgaggtpin5tests.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "aggbwsrsvm8c = avgaggtpin5tests.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "aggbwswsvm8c = avgaggtpin5tests.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "aggbwsrsvm16c = avgaggtpin5tests.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "aggbwswsvm16c = avgaggtpin5tests.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "aggbwsrmvm = avgaggtpin5tests.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "aggbwswmvm = avgaggtpin5tests.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n",
    "\n",
    "# Confidence Interval\n",
    "ciaggbwsrbf = ciaggtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "ciaggbwswbf = ciaggtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "ciaggbwsrsvm1c = ciaggtpin5tests.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "ciaggbwswsvm1c = ciaggtpin5tests.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "ciaggbwsrsvm2c = ciaggtpin5tests.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "ciaggbwswsvm2c = ciaggtpin5tests.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "ciaggbwsrsvm4c = ciaggtpin5tests.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "ciaggbwswsvm4c = ciaggtpin5tests.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "ciaggbwsrsvm8c = ciaggtpin5tests.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "ciaggbwswsvm8c = ciaggtpin5tests.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "ciaggbwsrsvm16c = ciaggtpin5tests.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "ciaggbwswsvm16c = ciaggtpin5tests.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "ciaggbwsrmvm = ciaggtpin5tests.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "ciaggbwswmvm = ciaggtpin5tests.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n",
    "\n",
    "OneThreadRead = [ aggbwsrbf[0], aggbwsrsvm1c[0], aggbwsrsvm2c[0], aggbwsrsvm4c[0], aggbwsrsvm8c[0], aggbwsrsvm16c[0] ]\n",
    "OneThreadWrite = [ aggbwswbf[0], aggbwswsvm1c[0], aggbwswsvm2c[0], aggbwswsvm4c[0], aggbwswsvm8c[0], aggbwswsvm16c[0] ]\n",
    "OneThreadWrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 12, 12  # change default size of figures\n",
    "plt.subplot(321)\n",
    "plt.plot(x_axis, bwsrbf)\n",
    "plt.plot(x_axis, bwsrsvm1c) \n",
    "plt.plot(x_axis, bwsrsvm2c)\n",
    "plt.plot(x_axis, bwsrsvm4c)\n",
    "plt.plot(x_axis, bwsrsvm8c)\n",
    "plt.plot(x_axis, bwsrsvm16c)\n",
    "plt.plot(x_axis, bwsrmvm)\n",
    "#plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.plot(x_axis, bwsrbf, label = 'bare-metal total throughput')\n",
    "plt.plot(x_axis, bwswsvm1c, label = 'SVM-MT avg throughput with 1 core') \n",
    "plt.plot(x_axis, bwswsvm2c, label = 'SVM-MT avg throughput with 2 cores')\n",
    "plt.plot(x_axis, bwswsvm4c, label = 'SVM-MT avg throughput with 4 cores')\n",
    "plt.plot(x_axis, bwswsvm8c, label = 'SVM-MT avg throughput with 8 cores')\n",
    "plt.plot(x_axis, bwswsvm16c, label = 'SVM-MT avg throughput with 16 cores')\n",
    "plt.plot(x_axis, bwsrmvm, label = 'MVM-ST avg throughput with 1 core')\n",
    "#plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Sequential Write')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "#pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.errorbar(x_axis, aggbwsrbf, ciaggbwsrbf)\n",
    "plt.errorbar(x_axis, aggbwsrsvm1c, ciaggbwsrsvm1c)\n",
    "plt.errorbar(x_axis, aggbwsrsvm2c, ciaggbwsrsvm2c)\n",
    "plt.errorbar(x_axis, aggbwsrsvm4c, ciaggbwsrsvm4c)\n",
    "plt.errorbar(x_axis, aggbwsrsvm8c, ciaggbwsrsvm8c)\n",
    "plt.errorbar(x_axis, aggbwsrsvm16c, ciaggbwsrsvm16c)\n",
    "plt.errorbar(x_axis, aggbwsrmvm, ciaggbwsrmvm)\n",
    "plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(324)\n",
    "plt.errorbar(x_axis, aggbwswbf, ciaggbwswbf, label = 'bare-metal total throughput')\n",
    "plt.errorbar(x_axis, aggbwswsvm1c, ciaggbwswsvm1c, label = 'total th.put SVM-MT with 1 core')\n",
    "plt.errorbar(x_axis, aggbwswsvm2c, ciaggbwswsvm2c, label = 'total th.put SVM-MT with 2 cores')\n",
    "plt.errorbar(x_axis, aggbwswsvm4c, ciaggbwswsvm4c, label = 'total th.put SVM-MT with 4 cores')\n",
    "plt.errorbar(x_axis, aggbwswsvm8c, ciaggbwswsvm8c, label = 'total th.put SVM-MT with 8 cores')\n",
    "plt.errorbar(x_axis, aggbwswsvm16c, ciaggbwswsvm16c, label = 'total th.put SVM-MT with 16 cores')\n",
    "plt.errorbar(x_axis, aggbwswmvm, ciaggbwswmvm, label = 'total th.put MVM-ST with 1 core')\n",
    "plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "#plt.savefig('throughput16procs.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 12, 6  # change default size of figures\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.errorbar(x_axis, aggbwsrbf, ciaggbwsrbf)\n",
    "plt.errorbar(x_axis, aggbwsrsvm1c, ciaggbwsrsvm1c)\n",
    "plt.errorbar(x_axis, aggbwsrsvm2c, ciaggbwsrsvm2c)\n",
    "plt.errorbar(x_axis, aggbwsrsvm4c, ciaggbwsrsvm4c)\n",
    "plt.errorbar(x_axis, aggbwsrsvm8c, ciaggbwsrsvm8c)\n",
    "plt.errorbar(x_axis, aggbwsrsvm16c, ciaggbwsrsvm16c)\n",
    "plt.errorbar(x_axis, aggbwsrmvm, ciaggbwsrmvm)\n",
    "plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.errorbar(x_axis, aggbwswbf, ciaggbwswbf, label = 'bare-metal')\n",
    "plt.errorbar(x_axis, aggbwswsvm1c, ciaggbwswsvm1c, label = 'SVM-MT 1C')\n",
    "plt.errorbar(x_axis, aggbwswsvm2c, ciaggbwswsvm2c, label = 'SVM-MT 2C')\n",
    "plt.errorbar(x_axis, aggbwswsvm4c, ciaggbwswsvm4c, label = 'SVM-MT 4C')\n",
    "plt.errorbar(x_axis, aggbwswsvm8c, ciaggbwswsvm8c, label = 'SVM-MT 8C')\n",
    "plt.errorbar(x_axis, aggbwswsvm16c, ciaggbwswsvm16c, label = 'SVM-MT 16C')\n",
    "plt.errorbar(x_axis, aggbwswmvm, ciaggbwswmvm, label = 'MVM-ST 1C')\n",
    "plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.legend(bbox_to_anchor=(0.95, 0.1), loc=4, borderaxespad=0.)\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.savefig('../figures/throughput_cfq.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 4, 4  # change default size of figures\n",
    "\n",
    "plt.errorbar(x_axis, aggbwsrbf, ciaggbwsrbf)\n",
    "plt.errorbar(x_axis, aggbwsrsvm1c, ciaggbwsrsvm1c)\n",
    "plt.errorbar(x_axis, aggbwsrsvm2c, ciaggbwsrsvm2c)\n",
    "plt.errorbar(x_axis, aggbwsrsvm4c, ciaggbwsrsvm4c)\n",
    "plt.errorbar(x_axis, aggbwsrsvm8c, ciaggbwsrsvm8c)\n",
    "plt.errorbar(x_axis, aggbwsrsvm16c, ciaggbwsrsvm16c)\n",
    "plt.errorbar(x_axis, aggbwsrmvm, ciaggbwsrmvm)\n",
    "plt.axis([0.9, 16.1, 0, 120])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "plt.savefig('throughput_cfq_read.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 4, 4  # change default size of figures\n",
    "plt.errorbar(x_axis, aggbwswbf, ciaggbwswbf, label = 'bare-metal')\n",
    "plt.errorbar(x_axis, aggbwswsvm1c, ciaggbwswsvm1c, label = 'SVM-MT 1C')\n",
    "plt.errorbar(x_axis, aggbwswsvm2c, ciaggbwswsvm2c, label = 'SVM-MT 2C')\n",
    "plt.errorbar(x_axis, aggbwswsvm4c, ciaggbwswsvm4c, label = 'SVM-MT 4C')\n",
    "plt.errorbar(x_axis, aggbwswsvm8c, ciaggbwswsvm8c, label = 'SVM-MT 8C')\n",
    "plt.errorbar(x_axis, aggbwswsvm16c, ciaggbwswsvm16c, label = 'SVM-MT 16C')\n",
    "plt.errorbar(x_axis, aggbwswmvm, ciaggbwswmvm, label = 'MVM-ST 1C')\n",
    "plt.axis([0.9, 16.1, 0, 120])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "#plt.legend(bbox_to_anchor=(0.4, 0.01), loc=4, prop={'size':8}, borderaxespad=0.)\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.savefig('throughput_cfq_write.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting using median\n",
    "# median of 5 tests\n",
    "mediantpin1test = dtdata.T.median() # Average throughput of each thread/VM in one test\n",
    "mediantpin5tests = mediantpin1test.groupby(level=0).median() # Average throughput of each thread/VM in 5 tests\n",
    "mediantpin5tests = mediantpin5tests / 1024 # Converting to Megabytes/second\n",
    "\n",
    "# Putting median throughput per thread/VM to vector before plotting\n",
    "\n",
    "medbwsrbf = mediantpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "medbwswbf = mediantpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "medbwsrsvm1c = mediantpin5tests.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "medbwswsvm1c = mediantpin5tests.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "medbwsrsvm2c = mediantpin5tests.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "medbwswsvm2c = mediantpin5tests.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "medbwsrsvm4c = mediantpin5tests.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "medbwswsvm4c = mediantpin5tests.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "medbwsrsvm8c = mediantpin5tests.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "medbwswsvm8c = mediantpin5tests.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "medbwsrsvm16c = mediantpin5tests.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "medbwswsvm16c = mediantpin5tests.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "\n",
    "medbwsrmvm = mediantpin5tests.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "medbwswmvm = mediantpin5tests.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n",
    "\n",
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 12, 8  # change default size of figures\n",
    "plt.subplot(321)\n",
    "plt.plot(x_axis, medbwsrbf)\n",
    "plt.plot(x_axis, medbwsrsvm1c) \n",
    "plt.plot(x_axis, medbwsrsvm2c)\n",
    "plt.plot(x_axis, medbwsrsvm4c)\n",
    "plt.plot(x_axis, medbwsrsvm8c)\n",
    "plt.plot(x_axis, medbwsrsvm16c)\n",
    "plt.plot(x_axis, medbwsrmvm)\n",
    "#plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.plot(x_axis, medbwsrbf, label = 'bare-metal median throughput')\n",
    "plt.plot(x_axis, medbwswsvm1c, label = 'SVM-MT median throughput with 1 core') \n",
    "plt.plot(x_axis, medbwswsvm2c, label = 'SVM-MT median throughput with 2 cores')\n",
    "plt.plot(x_axis, medbwswsvm4c, label = 'SVM-MT median throughput with 4 cores')\n",
    "plt.plot(x_axis, medbwswsvm8c, label = 'SVM-MT median throughput with 8 cores')\n",
    "plt.plot(x_axis, medbwswsvm16c, label = 'SVM-MT median throughput with 16 cores')\n",
    "plt.plot(x_axis, medbwsrmvm, label = 'MVM-ST median throughput with 1 core')\n",
    "plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Sequential Write')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bare-metal with diferent number of online cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting figure for baremetal with different number of online core\n",
    "\n",
    "# Average throughput or througput per thread\n",
    "bwsrbf1c = avgtpin5tests.filter(regex=\"^SRBF01CTH\").tolist()\n",
    "bwswbf1c = avgtpin5tests.filter(regex=\"^SWBF01CTH\").tolist()\n",
    "bwsrbf2c = avgtpin5tests.filter(regex=\"^SRBF02CTH\").tolist()\n",
    "bwswbf2c = avgtpin5tests.filter(regex=\"^SWBF02CTH\").tolist()\n",
    "bwsrbf4c = avgtpin5tests.filter(regex=\"^SRBF04CTH\").tolist()\n",
    "bwswbf4c = avgtpin5tests.filter(regex=\"^SWBF04CTH\").tolist()\n",
    "bwsrbf8c = avgtpin5tests.filter(regex=\"^SRBF08CTH\").tolist()\n",
    "bwswbf8c = avgtpin5tests.filter(regex=\"^SWBF08CTH\").tolist()\n",
    "bwsrbf16c = avgtpin5tests.filter(regex=\"^SRBF16CTH\").tolist()\n",
    "bwswbf16c = avgtpin5tests.filter(regex=\"^SWBF16CTH\").tolist()\n",
    "bwsrbf32c = avgtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "bwswbf32c = avgtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "# Aggregated throughput or total throughput per test\n",
    "aggbwsrbf1c = avgaggtpin5tests.filter(regex=\"^SRBF01CTH\").tolist()\n",
    "aggbwswbf1c = avgaggtpin5tests.filter(regex=\"^SWBF01CTH\").tolist()\n",
    "aggbwsrbf2c = avgaggtpin5tests.filter(regex=\"^SRBF02CTH\").tolist()\n",
    "aggbwswbf2c = avgaggtpin5tests.filter(regex=\"^SWBF02CTH\").tolist()\n",
    "aggbwsrbf4c = avgaggtpin5tests.filter(regex=\"^SRBF04CTH\").tolist()\n",
    "aggbwswbf4c = avgaggtpin5tests.filter(regex=\"^SWBF04CTH\").tolist()\n",
    "aggbwsrbf8c = avgaggtpin5tests.filter(regex=\"^SRBF08CTH\").tolist()\n",
    "aggbwswbf8c = avgaggtpin5tests.filter(regex=\"^SWBF08CTH\").tolist()\n",
    "aggbwsrbf16c = avgaggtpin5tests.filter(regex=\"^SRBF16CTH\").tolist()\n",
    "aggbwswbf16c = avgaggtpin5tests.filter(regex=\"^SWBF16CTH\").tolist()\n",
    "aggbwsrbf32c = avgaggtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "aggbwswbf32c = avgaggtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "# Confidence Interval\n",
    "ciaggbwsrbf1c = ciaggtpin5tests.filter(regex=\"^SRBF01CTH\").tolist()\n",
    "ciaggbwswbf1c = ciaggtpin5tests.filter(regex=\"^SWBF01CTH\").tolist()\n",
    "ciaggbwsrbf2c = ciaggtpin5tests.filter(regex=\"^SRBF02CTH\").tolist()\n",
    "ciaggbwswbf2c = ciaggtpin5tests.filter(regex=\"^SWBF02CTH\").tolist()\n",
    "ciaggbwsrbf4c = ciaggtpin5tests.filter(regex=\"^SRBF04CTH\").tolist()\n",
    "ciaggbwswbf4c = ciaggtpin5tests.filter(regex=\"^SWBF04CTH\").tolist()\n",
    "ciaggbwsrbf8c = ciaggtpin5tests.filter(regex=\"^SRBF08CTH\").tolist()\n",
    "ciaggbwswbf8c = ciaggtpin5tests.filter(regex=\"^SWBF08CTH\").tolist()\n",
    "ciaggbwsrbf16c = ciaggtpin5tests.filter(regex=\"^SRBF16CTH\").tolist()\n",
    "ciaggbwswbf16c = ciaggtpin5tests.filter(regex=\"^SWBF16CTH\").tolist()\n",
    "ciaggbwsrbf32c = ciaggtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "ciaggbwswbf32c = ciaggtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 12, 12  # change default size of figures\n",
    "plt.subplot(321)\n",
    "plt.plot(x_axis, bwsrbf1c)\n",
    "plt.plot(x_axis, bwsrbf2c)\n",
    "plt.plot(x_axis, bwsrbf4c)\n",
    "plt.plot(x_axis, bwsrbf8c)\n",
    "plt.plot(x_axis, bwsrbf16c)\n",
    "plt.plot(x_axis, bwsrbf32c)\n",
    "#plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.plot(x_axis, bwsrbf1c, label = 'bare-metal 1 core average throughput')\n",
    "plt.plot(x_axis, bwsrbf2c, label = 'bare-metal 2 cores average throughput')\n",
    "plt.plot(x_axis, bwsrbf4c, label = 'bare-metal 4 cores average throughput')\n",
    "plt.plot(x_axis, bwsrbf8c, label = 'bare-metal 8 cores average throughput')\n",
    "plt.plot(x_axis, bwsrbf16c, label = 'bare-metal 16 cores average throughput')\n",
    "plt.plot(x_axis, bwsrbf32c, label = 'bare-metal 32 cores average throughput')\n",
    "#plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Sequential Write')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "#pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.errorbar(x_axis, aggbwsrbf1c, ciaggbwsrbf1c)\n",
    "plt.errorbar(x_axis, aggbwsrbf2c, ciaggbwsrbf2c)\n",
    "plt.errorbar(x_axis, aggbwsrbf4c, ciaggbwsrbf4c)\n",
    "plt.errorbar(x_axis, aggbwsrbf8c, ciaggbwsrbf8c)\n",
    "plt.errorbar(x_axis, aggbwsrbf16c, ciaggbwsrbf16c)\n",
    "plt.errorbar(x_axis, aggbwsrbf32c, ciaggbwsrbf32c)\n",
    "plt.axis([0.9, 16.1, 0, 120])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(324)\n",
    "plt.errorbar(x_axis, aggbwswbf1c, ciaggbwswbf1c, label = 'bare-metal 1 core total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf2c, ciaggbwswbf2c, label = 'bare-metal 2 cores total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf4c, ciaggbwswbf4c, label = 'bare-metal 4 cores total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf8c, ciaggbwswbf8c, label = 'bare-metal 8 cores total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf16c, ciaggbwswbf16c, label = 'bare-metal 16 cores total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf32c, ciaggbwswbf32c, label = 'bare-metal 32 cores total throughput')\n",
    "plt.axis([0.9, 16.1, 0, 120])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the effect of the number of CPU core on disk IO, we try reducing the number of online cores on the baremetal system to see whether we get the same discrepancy like the case of VM with different number of cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bare-metal with cfq scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting figure for baremetal with different number of online core and iodepth of 256\n",
    "bmresult = pd.read_csv('data/bw16p4gbf12cache_cfq.csv', delimiter=' ', header=None)\n",
    "bmdata = bmresult[range(1,17)]\n",
    "bmindex = bmresult[0]\n",
    "bmdata.index = bmindex.str.replace('T[0-2][0-9]', '',case=False)\n",
    "\n",
    "# To calculate the average throughput, we first calculate average throughput each thread has in one test, then again take the\n",
    "# average of 5 tests\n",
    "bmavgtpin1test = bmdata.T.mean() # Average throughput of each thread/VM in one test\n",
    "bmavgtpin5tests = bmavgtpin1test.groupby(level=0).mean() # Average throughput of each thread/VM in 5 tests\n",
    "bmavgtpin5tests = bmavgtpin5tests / 1024 # Converting to Megabytes/second\n",
    "\n",
    "# Average throughput or througput per thread\n",
    "bwsrbf1c = bmavgtpin5tests.filter(regex=\"^SRBF01CTH\").tolist()\n",
    "bwswbf1c = bmavgtpin5tests.filter(regex=\"^SWBF01CTH\").tolist()\n",
    "bwsrbf2c = bmavgtpin5tests.filter(regex=\"^SRBF02CTH\").tolist()\n",
    "bwswbf2c = bmavgtpin5tests.filter(regex=\"^SWBF02CTH\").tolist()\n",
    "bwsrbf4c = bmavgtpin5tests.filter(regex=\"^SRBF04CTH\").tolist()\n",
    "bwswbf4c = bmavgtpin5tests.filter(regex=\"^SWBF04CTH\").tolist()\n",
    "bwsrbf8c = bmavgtpin5tests.filter(regex=\"^SRBF08CTH\").tolist()\n",
    "bwswbf8c = bmavgtpin5tests.filter(regex=\"^SWBF08CTH\").tolist()\n",
    "bwsrbf16c = bmavgtpin5tests.filter(regex=\"^SRBF16CTH\").tolist()\n",
    "bwswbf16c = bmavgtpin5tests.filter(regex=\"^SWBF16CTH\").tolist()\n",
    "bwsrbf32c = bmavgtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "bwswbf32c = bmavgtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "# Calculate aggregated throughput on each host of each test, then taking the average\n",
    "bmaggtpin1test = bmdata.T.sum() # Aggregate throughput on each host in one test\n",
    "bmavgaggtpintestgrouped = bmaggtpin1test.groupby(level=0)\n",
    "bmavgaggtpin5tests = bmavgaggtpintestgrouped.mean() # Then calculate the average of the total throughput through 5 tests\n",
    "bmavgaggtpin5tests = bmavgaggtpin5tests / 1024 # Convert to MB/s\n",
    "bmciaggtpin5tests = bmavgaggtpintestgrouped.aggregate(lambda x: np.std(x) / np.sqrt(x.count()) * 1.96) # Confident interval of 95%\n",
    "bmciaggtpin5tests = bmciaggtpin5tests / 1024 # Convert to same scale with mean\n",
    "\n",
    "# Aggregated throughput or total throughput per test\n",
    "aggbwsrbf1c = bmavgaggtpin5tests.filter(regex=\"^SRBF01CTH\").tolist()\n",
    "aggbwswbf1c = bmavgaggtpin5tests.filter(regex=\"^SWBF01CTH\").tolist()\n",
    "aggbwsrbf2c = bmavgaggtpin5tests.filter(regex=\"^SRBF02CTH\").tolist()\n",
    "aggbwswbf2c = bmavgaggtpin5tests.filter(regex=\"^SWBF02CTH\").tolist()\n",
    "aggbwsrbf4c = bmavgaggtpin5tests.filter(regex=\"^SRBF04CTH\").tolist()\n",
    "aggbwswbf4c = bmavgaggtpin5tests.filter(regex=\"^SWBF04CTH\").tolist()\n",
    "aggbwsrbf8c = bmavgaggtpin5tests.filter(regex=\"^SRBF08CTH\").tolist()\n",
    "aggbwswbf8c = bmavgaggtpin5tests.filter(regex=\"^SWBF08CTH\").tolist()\n",
    "aggbwsrbf16c = bmavgaggtpin5tests.filter(regex=\"^SRBF16CTH\").tolist()\n",
    "aggbwswbf16c = bmavgaggtpin5tests.filter(regex=\"^SWBF16CTH\").tolist()\n",
    "aggbwsrbf32c = bmavgaggtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "aggbwswbf32c = bmavgaggtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "# Confidence Interval\n",
    "ciaggbwsrbf1c = bmciaggtpin5tests.filter(regex=\"^SRBF01CTH\").tolist()\n",
    "ciaggbwswbf1c = bmciaggtpin5tests.filter(regex=\"^SWBF01CTH\").tolist()\n",
    "ciaggbwsrbf2c = bmciaggtpin5tests.filter(regex=\"^SRBF02CTH\").tolist()\n",
    "ciaggbwswbf2c = bmciaggtpin5tests.filter(regex=\"^SWBF02CTH\").tolist()\n",
    "ciaggbwsrbf4c = bmciaggtpin5tests.filter(regex=\"^SRBF04CTH\").tolist()\n",
    "ciaggbwswbf4c = bmciaggtpin5tests.filter(regex=\"^SWBF04CTH\").tolist()\n",
    "ciaggbwsrbf8c = bmciaggtpin5tests.filter(regex=\"^SRBF08CTH\").tolist()\n",
    "ciaggbwswbf8c = bmciaggtpin5tests.filter(regex=\"^SWBF08CTH\").tolist()\n",
    "ciaggbwsrbf16c = bmciaggtpin5tests.filter(regex=\"^SRBF16CTH\").tolist()\n",
    "ciaggbwswbf16c = bmciaggtpin5tests.filter(regex=\"^SWBF16CTH\").tolist()\n",
    "ciaggbwsrbf32c = bmciaggtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "ciaggbwswbf32c = bmciaggtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 12, 12  # change default size of figures\n",
    "plt.subplot(321)\n",
    "plt.plot(x_axis, bwsrbf1c)\n",
    "plt.plot(x_axis, bwsrbf2c)\n",
    "plt.plot(x_axis, bwsrbf4c)\n",
    "plt.plot(x_axis, bwsrbf8c)\n",
    "plt.plot(x_axis, bwsrbf16c)\n",
    "plt.plot(x_axis, bwsrbf32c)\n",
    "#plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.plot(x_axis, bwsrbf1c, label = 'bare-metal 1 core avg. throughput')\n",
    "plt.plot(x_axis, bwsrbf2c, label = 'bare-metal 2 cores avg. throughput')\n",
    "plt.plot(x_axis, bwsrbf4c, label = 'bare-metal 4 cores avg. throughput')\n",
    "plt.plot(x_axis, bwsrbf8c, label = 'bare-metal 8 cores avg. throughput')\n",
    "plt.plot(x_axis, bwsrbf16c, label = 'bare-metal 16 cores avg. throughput')\n",
    "plt.plot(x_axis, bwsrbf32c, label = 'bare-metal 32 cores avg. throughput')\n",
    "#plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Sequential Write')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "#pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.errorbar(x_axis, aggbwsrbf1c, ciaggbwsrbf1c)\n",
    "plt.errorbar(x_axis, aggbwsrbf2c, ciaggbwsrbf2c)\n",
    "plt.errorbar(x_axis, aggbwsrbf4c, ciaggbwsrbf4c)\n",
    "plt.errorbar(x_axis, aggbwsrbf8c, ciaggbwsrbf8c)\n",
    "plt.errorbar(x_axis, aggbwsrbf16c, ciaggbwsrbf16c)\n",
    "plt.errorbar(x_axis, aggbwsrbf32c, ciaggbwsrbf32c)\n",
    "plt.axis([0.9, 16.1, 0, 120])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(324)\n",
    "plt.errorbar(x_axis, aggbwswbf1c, ciaggbwswbf1c, label = 'bare-metal 1 core total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf2c, ciaggbwswbf2c, label = 'bare-metal 2 cores total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf4c, ciaggbwswbf4c, label = 'bare-metal 4 cores total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf8c, ciaggbwswbf8c, label = 'bare-metal 8 cores total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf16c, ciaggbwswbf16c, label = 'bare-metal 16 cores total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf32c, ciaggbwswbf32c, label = 'bare-metal 32 cores total throughput')\n",
    "plt.axis([0.9, 16.1, 0, 120])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By chance I found out that the scheduler that we use for disk IO is deadline scheduler, not the default CFQ that we are using in the VM. The result here repeat the result that we have in the early version of the test. This leads to an unavoidable task is to rerun the measurement for virtualization (and to try other combination of VM scheduler and host scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting figure for baremetal with different number of online core and iodepth of 256\n",
    "bmresult = pd.read_csv('data/bw16p4gbf12cache_cfq.csv', delimiter=' ', header=None)\n",
    "bmdata = bmresult[range(1,17)]\n",
    "bmindex = bmresult[0]\n",
    "bmdata.index = bmindex.str.replace('T[0-2][0-9]', '',case=False)\n",
    "\n",
    "# To calculate the average throughput, we first calculate average throughput each thread has in one test, then again take the\n",
    "# average of 5 tests\n",
    "bmavgtpin1test = bmdata.T.mean() # Average throughput of each thread/VM in one test\n",
    "bmavgtpin5tests = bmavgtpin1test.groupby(level=0).mean() # Average throughput of each thread/VM in 5 tests\n",
    "bmavgtpin5tests = bmavgtpin5tests / 1024 # Converting to Megabytes/second\n",
    "\n",
    "# Average throughput or througput per thread\n",
    "bwsrbf1c = bmavgtpin5tests.filter(regex=\"^SRBF01CTH\").tolist()\n",
    "bwswbf1c = bmavgtpin5tests.filter(regex=\"^SWBF01CTH\").tolist()\n",
    "bwsrbf2c = bmavgtpin5tests.filter(regex=\"^SRBF02CTH\").tolist()\n",
    "bwswbf2c = bmavgtpin5tests.filter(regex=\"^SWBF02CTH\").tolist()\n",
    "bwsrbf4c = bmavgtpin5tests.filter(regex=\"^SRBF04CTH\").tolist()\n",
    "bwswbf4c = bmavgtpin5tests.filter(regex=\"^SWBF04CTH\").tolist()\n",
    "bwsrbf8c = bmavgtpin5tests.filter(regex=\"^SRBF08CTH\").tolist()\n",
    "bwswbf8c = bmavgtpin5tests.filter(regex=\"^SWBF08CTH\").tolist()\n",
    "bwsrbf16c = bmavgtpin5tests.filter(regex=\"^SRBF16CTH\").tolist()\n",
    "bwswbf16c = bmavgtpin5tests.filter(regex=\"^SWBF16CTH\").tolist()\n",
    "bwsrbf32c = bmavgtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "bwswbf32c = bmavgtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "# Calculate aggregated throughput on each host of each test, then taking the average\n",
    "bmaggtpin1test = bmdata.T.sum() # Aggregate throughput on each host in one test\n",
    "bmavgaggtpintestgrouped = bmaggtpin1test.groupby(level=0)\n",
    "bmavgaggtpin5tests = bmavgaggtpintestgrouped.mean() # Then calculate the average of the total throughput through 5 tests\n",
    "bmavgaggtpin5tests = bmavgaggtpin5tests / 1024 # Convert to MB/s\n",
    "bmciaggtpin5tests = bmavgaggtpintestgrouped.aggregate(lambda x: np.std(x) / np.sqrt(x.count()) * 1.96) # Confident interval of 95%\n",
    "bmciaggtpin5tests = bmciaggtpin5tests / 1024 # Convert to same scale with mean\n",
    "\n",
    "# Aggregated throughput or total throughput per test\n",
    "aggbwsrbf1c = bmavgaggtpin5tests.filter(regex=\"^SRBF01CTH\").tolist()\n",
    "aggbwswbf1c = bmavgaggtpin5tests.filter(regex=\"^SWBF01CTH\").tolist()\n",
    "aggbwsrbf2c = bmavgaggtpin5tests.filter(regex=\"^SRBF02CTH\").tolist()\n",
    "aggbwswbf2c = bmavgaggtpin5tests.filter(regex=\"^SWBF02CTH\").tolist()\n",
    "aggbwsrbf4c = bmavgaggtpin5tests.filter(regex=\"^SRBF04CTH\").tolist()\n",
    "aggbwswbf4c = bmavgaggtpin5tests.filter(regex=\"^SWBF04CTH\").tolist()\n",
    "aggbwsrbf8c = bmavgaggtpin5tests.filter(regex=\"^SRBF08CTH\").tolist()\n",
    "aggbwswbf8c = bmavgaggtpin5tests.filter(regex=\"^SWBF08CTH\").tolist()\n",
    "aggbwsrbf16c = bmavgaggtpin5tests.filter(regex=\"^SRBF16CTH\").tolist()\n",
    "aggbwswbf16c = bmavgaggtpin5tests.filter(regex=\"^SWBF16CTH\").tolist()\n",
    "aggbwsrbf32c = bmavgaggtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "aggbwswbf32c = bmavgaggtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "# Confidence Interval\n",
    "ciaggbwsrbf1c = bmciaggtpin5tests.filter(regex=\"^SRBF01CTH\").tolist()\n",
    "ciaggbwswbf1c = bmciaggtpin5tests.filter(regex=\"^SWBF01CTH\").tolist()\n",
    "ciaggbwsrbf2c = bmciaggtpin5tests.filter(regex=\"^SRBF02CTH\").tolist()\n",
    "ciaggbwswbf2c = bmciaggtpin5tests.filter(regex=\"^SWBF02CTH\").tolist()\n",
    "ciaggbwsrbf4c = bmciaggtpin5tests.filter(regex=\"^SRBF04CTH\").tolist()\n",
    "ciaggbwswbf4c = bmciaggtpin5tests.filter(regex=\"^SWBF04CTH\").tolist()\n",
    "ciaggbwsrbf8c = bmciaggtpin5tests.filter(regex=\"^SRBF08CTH\").tolist()\n",
    "ciaggbwswbf8c = bmciaggtpin5tests.filter(regex=\"^SWBF08CTH\").tolist()\n",
    "ciaggbwsrbf16c = bmciaggtpin5tests.filter(regex=\"^SRBF16CTH\").tolist()\n",
    "ciaggbwswbf16c = bmciaggtpin5tests.filter(regex=\"^SWBF16CTH\").tolist()\n",
    "ciaggbwsrbf32c = bmciaggtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "ciaggbwswbf32c = bmciaggtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 8, 3  # change default size of figures\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.errorbar(x_axis, aggbwsrbf1c, ciaggbwsrbf1c)\n",
    "plt.errorbar(x_axis, aggbwsrbf2c, ciaggbwsrbf2c)\n",
    "plt.errorbar(x_axis, aggbwsrbf4c, ciaggbwsrbf4c)\n",
    "plt.errorbar(x_axis, aggbwsrbf8c, ciaggbwsrbf8c)\n",
    "plt.errorbar(x_axis, aggbwsrbf16c, ciaggbwsrbf16c)\n",
    "plt.errorbar(x_axis, aggbwsrbf32c, ciaggbwsrbf32c)\n",
    "plt.axis([0.9, 16.1, 60, 120])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.errorbar(x_axis, aggbwswbf1c, ciaggbwswbf1c, label = 'BM 1 online core')\n",
    "plt.errorbar(x_axis, aggbwswbf2c, ciaggbwswbf2c, label = 'BM 2 online cores')\n",
    "plt.errorbar(x_axis, aggbwswbf4c, ciaggbwswbf4c, label = 'BM 4 online cores')\n",
    "plt.errorbar(x_axis, aggbwswbf8c, ciaggbwswbf8c, label = 'BM 8 online cores')\n",
    "plt.errorbar(x_axis, aggbwswbf16c, ciaggbwswbf16c, label = 'BM 16 online cores')\n",
    "plt.errorbar(x_axis, aggbwswbf32c, ciaggbwswbf32c, label = 'BM 32 online cores')\n",
    "plt.axis([0.9, 16.1, 60, 120])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.legend(bbox_to_anchor=(1, 0.05), loc=4, prop={'size':9}, borderaxespad=0.)\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "#pylab.ylabel('throughput (MB/s)')\n",
    "plt.savefig('throughput_cfq_bmdiffonlinecore.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bare-metal with noop scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting figure for baremetal with different number of online core and iodepth of 256\n",
    "bmresult = pd.read_csv('data/bw16p4gbf12cache_noop.csv', delimiter=' ', header=None)\n",
    "bmdata = bmresult[range(1,17)]\n",
    "bmindex = bmresult[0]\n",
    "bmdata.index = bmindex.str.replace('T[0-2][0-9]', '',case=False)\n",
    "\n",
    "# To calculate the average throughput, we first calculate average throughput each thread has in one test, then again take the\n",
    "# average of 5 tests\n",
    "bmavgtpin1test = bmdata.T.mean() # Average throughput of each thread/VM in one test\n",
    "bmavgtpin5tests = bmavgtpin1test.groupby(level=0).mean() # Average throughput of each thread/VM in 5 tests\n",
    "bmavgtpin5tests = bmavgtpin5tests / 1024 # Converting to Megabytes/second\n",
    "\n",
    "# Average throughput or througput per thread\n",
    "bwsrbf1c = bmavgtpin5tests.filter(regex=\"^SRBF01CTH\").tolist()\n",
    "bwswbf1c = bmavgtpin5tests.filter(regex=\"^SWBF01CTH\").tolist()\n",
    "bwsrbf2c = bmavgtpin5tests.filter(regex=\"^SRBF02CTH\").tolist()\n",
    "bwswbf2c = bmavgtpin5tests.filter(regex=\"^SWBF02CTH\").tolist()\n",
    "bwsrbf4c = bmavgtpin5tests.filter(regex=\"^SRBF04CTH\").tolist()\n",
    "bwswbf4c = bmavgtpin5tests.filter(regex=\"^SWBF04CTH\").tolist()\n",
    "bwsrbf8c = bmavgtpin5tests.filter(regex=\"^SRBF08CTH\").tolist()\n",
    "bwswbf8c = bmavgtpin5tests.filter(regex=\"^SWBF08CTH\").tolist()\n",
    "bwsrbf16c = bmavgtpin5tests.filter(regex=\"^SRBF16CTH\").tolist()\n",
    "bwswbf16c = bmavgtpin5tests.filter(regex=\"^SWBF16CTH\").tolist()\n",
    "bwsrbf32c = bmavgtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "bwswbf32c = bmavgtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "# Calculate aggregated throughput on each host of each test, then taking the average\n",
    "bmaggtpin1test = bmdata.T.sum() # Aggregate throughput on each host in one test\n",
    "bmavgaggtpintestgrouped = bmaggtpin1test.groupby(level=0)\n",
    "bmavgaggtpin5tests = bmavgaggtpintestgrouped.mean() # Then calculate the average of the total throughput through 5 tests\n",
    "bmavgaggtpin5tests = bmavgaggtpin5tests / 1024 # Convert to MB/s\n",
    "bmciaggtpin5tests = bmavgaggtpintestgrouped.aggregate(lambda x: np.std(x) / np.sqrt(x.count()) * 1.96) # Confident interval of 95%\n",
    "bmciaggtpin5tests = bmciaggtpin5tests / 1024 # Convert to same scale with mean\n",
    "\n",
    "# Aggregated throughput or total throughput per test\n",
    "aggbwsrbf1c = bmavgaggtpin5tests.filter(regex=\"^SRBF01CTH\").tolist()\n",
    "aggbwswbf1c = bmavgaggtpin5tests.filter(regex=\"^SWBF01CTH\").tolist()\n",
    "aggbwsrbf2c = bmavgaggtpin5tests.filter(regex=\"^SRBF02CTH\").tolist()\n",
    "aggbwswbf2c = bmavgaggtpin5tests.filter(regex=\"^SWBF02CTH\").tolist()\n",
    "aggbwsrbf4c = bmavgaggtpin5tests.filter(regex=\"^SRBF04CTH\").tolist()\n",
    "aggbwswbf4c = bmavgaggtpin5tests.filter(regex=\"^SWBF04CTH\").tolist()\n",
    "aggbwsrbf8c = bmavgaggtpin5tests.filter(regex=\"^SRBF08CTH\").tolist()\n",
    "aggbwswbf8c = bmavgaggtpin5tests.filter(regex=\"^SWBF08CTH\").tolist()\n",
    "aggbwsrbf16c = bmavgaggtpin5tests.filter(regex=\"^SRBF16CTH\").tolist()\n",
    "aggbwswbf16c = bmavgaggtpin5tests.filter(regex=\"^SWBF16CTH\").tolist()\n",
    "aggbwsrbf32c = bmavgaggtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "aggbwswbf32c = bmavgaggtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "# Confidence Interval\n",
    "ciaggbwsrbf1c = bmciaggtpin5tests.filter(regex=\"^SRBF01CTH\").tolist()\n",
    "ciaggbwswbf1c = bmciaggtpin5tests.filter(regex=\"^SWBF01CTH\").tolist()\n",
    "ciaggbwsrbf2c = bmciaggtpin5tests.filter(regex=\"^SRBF02CTH\").tolist()\n",
    "ciaggbwswbf2c = bmciaggtpin5tests.filter(regex=\"^SWBF02CTH\").tolist()\n",
    "ciaggbwsrbf4c = bmciaggtpin5tests.filter(regex=\"^SRBF04CTH\").tolist()\n",
    "ciaggbwswbf4c = bmciaggtpin5tests.filter(regex=\"^SWBF04CTH\").tolist()\n",
    "ciaggbwsrbf8c = bmciaggtpin5tests.filter(regex=\"^SRBF08CTH\").tolist()\n",
    "ciaggbwswbf8c = bmciaggtpin5tests.filter(regex=\"^SWBF08CTH\").tolist()\n",
    "ciaggbwsrbf16c = bmciaggtpin5tests.filter(regex=\"^SRBF16CTH\").tolist()\n",
    "ciaggbwswbf16c = bmciaggtpin5tests.filter(regex=\"^SWBF16CTH\").tolist()\n",
    "ciaggbwsrbf32c = bmciaggtpin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "ciaggbwswbf32c = bmciaggtpin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 12, 12  # change default size of figures\n",
    "plt.subplot(321)\n",
    "plt.plot(x_axis, bwsrbf1c)\n",
    "plt.plot(x_axis, bwsrbf2c)\n",
    "plt.plot(x_axis, bwsrbf4c)\n",
    "plt.plot(x_axis, bwsrbf8c)\n",
    "plt.plot(x_axis, bwsrbf16c)\n",
    "plt.plot(x_axis, bwsrbf32c)\n",
    "#plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.plot(x_axis, bwsrbf1c, label = 'bare-metal 1 core avg. throughput')\n",
    "plt.plot(x_axis, bwsrbf2c, label = 'bare-metal 2 cores avg. throughput')\n",
    "plt.plot(x_axis, bwsrbf4c, label = 'bare-metal 4 cores avg. throughput')\n",
    "plt.plot(x_axis, bwsrbf8c, label = 'bare-metal 8 cores avg. throughput')\n",
    "plt.plot(x_axis, bwsrbf16c, label = 'bare-metal 16 cores avg. throughput')\n",
    "plt.plot(x_axis, bwsrbf32c, label = 'bare-metal 32 cores avg. throughput')\n",
    "#plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Sequential Write')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "#pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.errorbar(x_axis, aggbwsrbf1c, ciaggbwsrbf1c)\n",
    "plt.errorbar(x_axis, aggbwsrbf2c, ciaggbwsrbf2c)\n",
    "plt.errorbar(x_axis, aggbwsrbf4c, ciaggbwsrbf4c)\n",
    "plt.errorbar(x_axis, aggbwsrbf8c, ciaggbwsrbf8c)\n",
    "plt.errorbar(x_axis, aggbwsrbf16c, ciaggbwsrbf16c)\n",
    "plt.errorbar(x_axis, aggbwsrbf32c, ciaggbwsrbf32c)\n",
    "plt.axis([0.9, 16.1, 0, 120])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')\n",
    "\n",
    "plt.subplot(324)\n",
    "plt.errorbar(x_axis, aggbwswbf1c, ciaggbwswbf1c, label = 'bare-metal 1 core total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf2c, ciaggbwswbf2c, label = 'bare-metal 2 cores total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf4c, ciaggbwswbf4c, label = 'bare-metal 4 cores total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf8c, ciaggbwswbf8c, label = 'bare-metal 8 cores total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf16c, ciaggbwswbf16c, label = 'bare-metal 16 cores total throughput')\n",
    "plt.errorbar(x_axis, aggbwswbf32c, ciaggbwswbf32c, label = 'bare-metal 32 cores total throughput')\n",
    "plt.axis([0.9, 16.1, 0, 120])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('throughput (MB/s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(throughput of bare-metal is used as baseline)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division # floating point division in Py2x\n",
    "# for throughput per thread/VM\n",
    "nml_bwsrbf = [x/y for x,y in zip(bwsrbf,bwsrbf)]\n",
    "nml_bwswbf = [x/y for x,y in zip(bwswbf,bwswbf)]\n",
    "\n",
    "nml_bwsrsvm1c = [x/y for x,y in zip(bwsrsvm1c,bwsrbf)]\n",
    "nml_bwswsvm1c = [x/y for x,y in zip(bwswsvm1c,bwswbf)]\n",
    "nml_bwsrsvm2c = [x/y for x,y in zip(bwsrsvm2c,bwsrbf)]\n",
    "nml_bwswsvm2c = [x/y for x,y in zip(bwswsvm2c,bwswbf)]\n",
    "nml_bwsrsvm4c = [x/y for x,y in zip(bwsrsvm4c,bwsrbf)]\n",
    "nml_bwswsvm4c = [x/y for x,y in zip(bwswsvm4c,bwswbf)]\n",
    "nml_bwsrsvm8c = [x/y for x,y in zip(bwsrsvm8c,bwsrbf)]\n",
    "nml_bwswsvm8c = [x/y for x,y in zip(bwswsvm8c,bwswbf)]\n",
    "nml_bwsrsvm16c = [x/y for x,y in zip(bwsrsvm16c,bwsrbf)]\n",
    "nml_bwswsvm16c = [x/y for x,y in zip(bwswsvm16c,bwswbf)]\n",
    "\n",
    "nml_bwsrmvm = [x/y for x,y in zip(bwsrmvm,bwsrbf)]\n",
    "nml_bwswmvm = [x/y for x,y in zip(bwswmvm,bwswbf)]\n",
    "\n",
    "# for aggregated throughput\n",
    "nml_aggbwsrbf = [x/y for x,y in zip(aggbwsrbf,aggbwsrbf)]\n",
    "nml_aggbwswbf = [x/y for x,y in zip(aggbwswbf,aggbwswbf)]\n",
    "\n",
    "nml_aggbwsrsvm1c = [x/y for x,y in zip(aggbwsrsvm1c,aggbwsrbf)]\n",
    "nml_aggbwswsvm1c = [x/y for x,y in zip(aggbwswsvm1c,aggbwswbf)]\n",
    "nml_aggbwsrsvm2c = [x/y for x,y in zip(aggbwsrsvm2c,aggbwsrbf)]\n",
    "nml_aggbwswsvm2c = [x/y for x,y in zip(aggbwswsvm2c,aggbwswbf)]\n",
    "nml_aggbwsrsvm4c = [x/y for x,y in zip(aggbwsrsvm4c,aggbwsrbf)]\n",
    "nml_aggbwswsvm4c = [x/y for x,y in zip(aggbwswsvm4c,aggbwswbf)]\n",
    "nml_aggbwsrsvm8c = [x/y for x,y in zip(aggbwsrsvm8c,aggbwsrbf)]\n",
    "nml_aggbwswsvm8c = [x/y for x,y in zip(aggbwswsvm8c,aggbwswbf)]\n",
    "nml_aggbwsrsvm16c = [x/y for x,y in zip(aggbwsrsvm16c,aggbwsrbf)]\n",
    "nml_aggbwswsvm16c = [x/y for x,y in zip(aggbwswsvm16c,aggbwswbf)]\n",
    "\n",
    "nml_aggbwsrmvm = [x/y for x,y in zip(aggbwsrmvm,aggbwsrbf)]\n",
    "nml_aggbwswmvm = [x/y for x,y in zip(aggbwswmvm,aggbwswbf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = 10, 6  # change default size of figures\n",
    "x_axis = [1,2,4,8,16]\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(x_axis, nml_aggbwsrbf, label = 'bare-metal')\n",
    "plt.plot(x_axis, nml_aggbwsrsvm1c, label = 'SVM-MT 1C')\n",
    "plt.plot(x_axis, nml_aggbwsrsvm2c, label = 'SVM-MT 2C')\n",
    "plt.plot(x_axis, nml_aggbwsrsvm4c, label = 'SVM-MT 4C')\n",
    "plt.plot(x_axis, nml_aggbwsrsvm8c, label = 'SVM-MT 8C')\n",
    "plt.plot(x_axis, nml_aggbwsrsvm16c, label = 'SVM-MT 16C')\n",
    "plt.plot(x_axis, nml_aggbwsrmvm, label = 'MVM-ST')\n",
    "plt.axis([1, 16, 0, 5.5])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('percentage')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(x_axis, nml_aggbwswbf, label = 'bare-metal')\n",
    "plt.plot(x_axis, nml_aggbwswsvm1c, label = 'SVM-MT 1C')\n",
    "plt.plot(x_axis, nml_aggbwswsvm2c, label = 'SVM-MT 2C')\n",
    "plt.plot(x_axis, nml_aggbwswsvm4c, label = 'SVM-MT 4C')\n",
    "plt.plot(x_axis, nml_aggbwswsvm8c, label = 'SVM-MT 8C')\n",
    "plt.plot(x_axis, nml_aggbwswsvm16c, label = 'SVM-MT 16C')\n",
    "plt.plot(x_axis, nml_aggbwswmvm, label = 'MVM-ST')\n",
    "plt.axis([1, 16, 0, 1.5])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('percentage')\n",
    "\n",
    "#plt.savefig('nml_throughput16procs.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the overhead in compare to the baremetal, we will look at the normalization of the aggregated throughput of the three platforms: bare-metal, single VM with multiple threads, and multiple VMs with single thread. We set the throughput of baremetal as the base and compare with the VM ones.\n",
    "\n",
    "To answer these questions, we will look into other metrics of the experiments: Jain Fairness Index, CPU utilisation, Disk Utilization, Context Switching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jain Fairness Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jfi = dtdata.T.sum()**2/dtdata.T.count()/(dtdata**2).T.sum() #Calculate Jair Fairness Index for each test\n",
    "jfigrouped = jfi.groupby(level=0)\n",
    "jfiavg = jfigrouped.mean() # Calculate JFI average from 5 tests\n",
    "\n",
    "# for plotting confident interval\n",
    "# jfistd = jfigrouped.std() # Standard Deviation\n",
    "cijfi = jfigrouped.aggregate(lambda x: np.std(x) / np.sqrt(x.count()) * 1.96) # Confident interval of 95%\n",
    "\n",
    "# prepare for plotting\n",
    "jfisrbf = jfiavg.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "jfiswbf = jfiavg.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "jfisrsvm1c = jfiavg.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "jfiswsvm1c = jfiavg.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "jfisrsvm2c = jfiavg.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "jfiswsvm2c = jfiavg.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "jfisrsvm4c = jfiavg.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "jfiswsvm4c = jfiavg.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "jfisrsvm8c = jfiavg.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "jfiswsvm8c = jfiavg.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "jfisrsvm16c = jfiavg.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "jfiswsvm16c = jfiavg.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "\n",
    "jfisrmvm = jfiavg.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "jfiswmvm = jfiavg.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n",
    "\n",
    "\n",
    "cijfisrbf = cijfi.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "cijfiswbf = cijfi.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "cijfisrsvm1c = cijfi.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "cijfiswsvm1c = cijfi.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "cijfisrsvm2c = cijfi.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "cijfiswsvm2c = cijfi.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "cijfisrsvm4c = cijfi.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "cijfiswsvm4c = cijfi.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "cijfisrsvm8c = cijfi.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "cijfiswsvm8c = cijfi.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "cijfisrsvm16c = cijfi.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "cijfiswsvm16c = cijfi.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "\n",
    "cijfisrmvm = cijfi.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "cijfiswmvm = cijfi.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = 9, 4  # change default size of figures\n",
    "x_axis = [1,2,4,8,16]\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.errorbar(x_axis, jfisrbf, cijfisrbf, label = 'bare-metal')\n",
    "plt.errorbar(x_axis, jfisrsvm1c, cijfisrsvm1c, label = 'SVM-MT 1C')\n",
    "plt.errorbar(x_axis, jfisrsvm2c, cijfisrsvm2c, label = 'SVM-MT 2C')\n",
    "plt.errorbar(x_axis, jfisrsvm4c, cijfisrsvm4c, label = 'SVM-MT 4C')\n",
    "plt.errorbar(x_axis, jfisrsvm8c, cijfisrsvm8c, label = 'SVM-MT 8C')\n",
    "plt.errorbar(x_axis, jfisrsvm16c, cijfisrsvm16c, label = 'SVM-MT 16C')\n",
    "plt.errorbar(x_axis, jfisrmvm, cijfisrmvm, label = 'MVM-ST')\n",
    "plt.axis([0.9, 16.1, 0.7, 1.03])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "plt.xlabel('concurrent threads/VMs')\n",
    "plt.ylabel('Jain Fairness Index')\n",
    "plt.xticks(x_axis)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.errorbar(x_axis, jfiswbf, cijfiswbf, label = 'bare-metal')\n",
    "plt.errorbar(x_axis, jfiswsvm1c, cijfiswsvm1c, label = 'SVM-MT 1C')\n",
    "plt.errorbar(x_axis, jfiswsvm2c, cijfiswsvm2c, label = 'SVM-MT 2C')\n",
    "plt.errorbar(x_axis, jfiswsvm4c, cijfiswsvm4c, label = 'SVM-MT 4C')\n",
    "plt.errorbar(x_axis, jfiswsvm8c, cijfiswsvm8c, label = 'SVM-MT 8C')\n",
    "plt.errorbar(x_axis, jfiswsvm16c, cijfiswsvm16c, label = 'SVM-MT 16C')\n",
    "plt.errorbar(x_axis, jfiswmvm, cijfiswmvm, label = 'MVM-ST')\n",
    "plt.axis([0.9, 16.1, 0.7, 1.03])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.xlabel('concurrent threads/VMs')\n",
    "plt.legend(bbox_to_anchor=(0.95, 0.05), loc=4, borderaxespad=0.)\n",
    "#plt.ylabel('JFI index')\n",
    "plt.xticks(x_axis)\n",
    "\n",
    "plt.savefig('JFI.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jain Fairness Index (JFI) has a range from 0 to 1 which indicates the level of fairness between concurrent threads. If the threads receive equal partition of bandwidth, we would achieve index of 1. If only k of n flows receive equal bandwidth (and others get none), index is k/n. In the worst case, JFI has the index of 0. We plot some figures of JFI for the three platforms with confident interval of 95%. The figures show that JFI is always 1 which means that threads have very perfectly equal throughput or fair to each others.\n",
    "CFQ scheduler use 64 queues when dealing synchronous request and only one queue when dealing with asynchronous request. The behaviors of baremetal can be because of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU Utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CPU utilization, FIO relies on _getrusage()_. _Getrusage()_ returns resource usage statistics for the calling thread.\n",
    "The resource usages are returned in the structure pointed by the usage caller, which has the following form:\n",
    "~~~\n",
    "struct rusage {\n",
    "    struct timeval ru_utime; /* user CPU time used */\n",
    "    struct timeval ru_stime; /* system CPU time used */\n",
    "    ...\n",
    "    long   ru_minflt;        /* page reclaims (soft page faults) */\n",
    "    long   ru_majflt;        /* page faults (hard page faults) */\n",
    "    long   ru_nswap;         /* swaps */\n",
    "    ...\n",
    "    long   ru_nvcsw;         /* voluntary context switches */\n",
    "    long   ru_nivcsw;        /* involuntary context switches */\n",
    "};\n",
    "~~~\n",
    "and contains inside the _struct_:\n",
    "* _ru_utime_: This is the total amount of time spent executing in user mode, expressed in a _timeval_ structure (seconds plus microseconds).\n",
    "* _ru_stime_: This is the total amount of time spent executing in kernel mode, expressed in a _timeval_ structure (seconds plus microseconds).\n",
    "* _ru_nvcsw_: The number of times a context switch resulted due to a process voluntarily giving up the processor before its time slice was completed (usually to await availability of a resource).\n",
    "* _ru_nivcsw_: The number of times a context switch resulted due to a higher priority process becoming runnable or because the current process exceeded its time slice.\n",
    "\n",
    "FIO reports the CPU utilization for each thread by (_user_time/total_run_time_) and (_system_time/total_run_time_), where:\n",
    "* _user_time_ is the duration that the thread spending in user space\n",
    "* _system_time_ is the duration the that the thread spending in kernel space\n",
    "* _total_run_time_ is the duration from the time that thread starts until the time that thread finishs or the wall lock time.\n",
    "\n",
    "~~~\n",
    "\truntime = ts->total_run_time;\n",
    "\tif (runtime) {\n",
    "\t\tdouble runt = (double) runtime;\n",
    "\n",
    "\t\tusr_cpu = (double) ts->usr_time * 100 / runt;\n",
    "\t\tsys_cpu = (double) ts->sys_time * 100 / runt;\n",
    "\t} else {\n",
    "\t\tusr_cpu = 0;\n",
    "\t\tsys_cpu = 0;\n",
    "\t}\n",
    "~~~\n",
    "In this report, we especially focus on the the serving time (the time that thread actually uses the CPU) and the waiting time (the time that thread has to wait for the others until its slice is up). Serving time and Waiting time were calculated by this formular:\n",
    "~~~\n",
    "    serving_time = user_time + kernel_time\n",
    "    waiting_time = total_thread_runtime - serving_time\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove percentage symbol in csv file first\n",
    "!sed -i 's/%//g' data/cpuus16p4gbf12cache_cfq.csv\n",
    "!sed -i 's/%//g' data/cpuks16p4gbf12cache_cfq.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load time spending in user space\n",
    "cpuusresult = pd.read_csv('data/cpuus16p4gbf12cache_cfq.csv', delimiter=' ', header=None)\n",
    "cpuusdata = cpuusresult[range(1,17)]\n",
    "cpuusindex = cpuusresult[0]\n",
    "cpuusdata.index = cpuusindex.str.replace('T[0-2][0-9]', '',case=False)\n",
    "# Load time spending in kernel space\n",
    "cpuksresult = pd.read_csv('data/cpuks16p4gbf12cache_cfq.csv', delimiter=' ', header=None)\n",
    "cpuksdata = cpuksresult[range(1,17)]\n",
    "cpuksindex = cpuksresult[0]\n",
    "cpuksdata.index = cpuksindex.str.replace('T[0-2][0-9]', '',case=False)\n",
    "# Load total runtime of thread\n",
    "threadruntimeresult = pd.read_csv('data/rt16p4gbf12cache_cfq.csv', delimiter=' ', header=None)\n",
    "threadruntimedata = threadruntimeresult[range(1,17)]\n",
    "threadruntimeindex = threadruntimeresult[0]\n",
    "threadruntimedata.index = threadruntimeindex.str.replace('T[0-2][0-9]', '',case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate duration that thread utilise the CPU\n",
    "# Real CPU utilisation duration (ms) = (user time (%) + system time (%)) * runtime (ms)\n",
    "totalcpupercentage = cpuusdata + cpuksdata\n",
    "totalcputime = totalcpupercentage.mul(threadruntimedata,1) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalcputimehostin1test = totalcputime.T.sum() # Sum of CPU serving time in one test\n",
    "totalcputime5testsgrouped = totalcputimehostin1test.groupby(level=0) \n",
    "totalcputimehostavgin5tests = totalcputime5testsgrouped.mean() # Mean of each thread/VM on 5 tests\n",
    "citct = totalcputime5testsgrouped.aggregate(lambda x: np.std(x) / np.sqrt(x.count()) * 1.96) # Confident interval of 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threadruntimehostin1test = threadruntimedata.T.sum() # Sum of thread running time in one test\n",
    "threadruntime5testsgrouped = threadruntimehostin1test.groupby(level=0) \n",
    "threadruntimehostavgin5tests = threadruntime5testsgrouped.mean() # Mean of each thread/VM on 5 tests\n",
    "citrt = threadruntime5testsgrouped.aggregate(lambda x: np.std(x) / np.sqrt(x.count()) * 1.96) # Confident interval of 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CPU serving time\n",
    "tctsrbf = totalcputimehostavgin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "tctswbf = totalcputimehostavgin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "tctsrsvm1c = totalcputimehostavgin5tests.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "tctswsvm1c = totalcputimehostavgin5tests.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "tctsrsvm2c = totalcputimehostavgin5tests.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "tctswsvm2c = totalcputimehostavgin5tests.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "tctsrsvm4c = totalcputimehostavgin5tests.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "tctswsvm4c = totalcputimehostavgin5tests.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "tctsrsvm8c = totalcputimehostavgin5tests.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "tctswsvm8c = totalcputimehostavgin5tests.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "tctsrsvm16c = totalcputimehostavgin5tests.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "tctswsvm16c = totalcputimehostavgin5tests.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "\n",
    "tctsrmvm = totalcputimehostavgin5tests.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "tctswmvm = totalcputimehostavgin5tests.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n",
    "\n",
    "\n",
    "# Confident interval\n",
    "citctsrbf = citct.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "citctswbf = citct.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "citctsrsvm1c = citct.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "citctswsvm1c = citct.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "citctsrsvm2c = citct.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "citctswsvm2c = citct.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "citctsrsvm4c = citct.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "citctswsvm4c = citct.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "citctsrsvm8c = citct.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "citctswsvm8c = citct.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "citctsrsvm16c = citct.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "citctswsvm16c = citct.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "\n",
    "citctsrmvm = citct.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "citctswmvm = citct.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n",
    "\n",
    "\n",
    "# Thread running time\n",
    "trtsrbf = threadruntimehostavgin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "trtswbf = threadruntimehostavgin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "\n",
    "trtsrsvm1c = threadruntimehostavgin5tests.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "trtswsvm1c = threadruntimehostavgin5tests.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "trtsrsvm2c = threadruntimehostavgin5tests.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "trtswsvm2c = threadruntimehostavgin5tests.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "trtsrsvm4c = threadruntimehostavgin5tests.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "trtswsvm4c = threadruntimehostavgin5tests.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "trtsrsvm8c = threadruntimehostavgin5tests.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "trtswsvm8c = threadruntimehostavgin5tests.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "trtsrsvm16c = threadruntimehostavgin5tests.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "trtswsvm16c = threadruntimehostavgin5tests.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "\n",
    "trtsrmvm = threadruntimehostavgin5tests.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "trtswmvm = threadruntimehostavgin5tests.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n",
    "\n",
    "\n",
    "# Confident interval\n",
    "citrtsrbf = citrt.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "citrtswbf = citrt.filter(regex=\"^SWBF32CTH\").tolist()\n",
    " \n",
    "citrtsrsvm1c = citrt.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "citrtswsvm1c = citct.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "citrtsrsvm2c = citrt.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "citrtswsvm2c = citct.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "citrtsrsvm4c = citrt.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "citrtswsvm4c = citct.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "citrtsrsvm8c = citrt.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "citrtswsvm8c = citct.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "citrtsrsvm16c = citrt.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "citrtswsvm16c = citct.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "\n",
    "citrtsrmvm = citrt.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "citrtswmvm = citrt.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 12, 8  # change default size of figures\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.errorbar(x_axis, tctsrbf, citctsrbf, color=\"blue\")\n",
    "plt.errorbar(x_axis, tctsrsvm1c, citctsrsvm1c, color=\"green\")\n",
    "plt.errorbar(x_axis, tctsrsvm2c, citctsrsvm2c, color=\"green\")\n",
    "plt.errorbar(x_axis, tctsrsvm4c, citctsrsvm4c, color=\"green\")\n",
    "plt.errorbar(x_axis, tctsrsvm8c, citctsrsvm8c, color=\"green\")\n",
    "plt.errorbar(x_axis, tctsrsvm16c, citctsrsvm16c, color=\"green\")\n",
    "plt.errorbar(x_axis, tctsrmvm, citctsrmvm, color=\"red\")\n",
    "plt.errorbar(x_axis, trtsrbf, citrtsrbf, ls=\"dashed\", color=\"blue\")\n",
    "plt.errorbar(x_axis, trtsrsvm1c, citrtsrsvm1c, ls=\"dashed\", color=\"green\")\n",
    "plt.errorbar(x_axis, trtsrsvm2c, citrtsrsvm2c, ls=\"dashed\", color=\"green\")\n",
    "plt.errorbar(x_axis, trtsrsvm4c, citrtsrsvm4c, ls=\"dashed\", color=\"green\")\n",
    "plt.errorbar(x_axis, trtsrsvm8c, citrtsrsvm8c, ls=\"dashed\", color=\"green\")\n",
    "plt.errorbar(x_axis, trtsrsvm16c, citrtsrsvm16c, ls=\"dashed\", color=\"green\")\n",
    "plt.errorbar(x_axis, trtsrmvm, citrtsrmvm, ls=\"dashed\", color=\"red\")\n",
    "#plt.axis([0.9, 16.1, 0, 110000])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "plt.xlabel('concurrent threads/VMs')\n",
    "plt.ylabel('duration (ms)')\n",
    "plt.xticks(x_axis)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.errorbar(x_axis, tctswbf, citctswbf, color=\"blue\", label = 'mean of aggr. CPU serving time bare-metal')\n",
    "plt.errorbar(x_axis, tctswsvm1c, citctswsvm1c, color=\"green\", label = 'mean of aggr. CPU serving time SVM-MT 1C')\n",
    "plt.errorbar(x_axis, tctswsvm2c, citctswsvm2c, color=\"green\", label = 'mean of aggr. CPU serving time SVM-MT 2C')\n",
    "plt.errorbar(x_axis, tctswsvm4c, citctswsvm4c, color=\"green\", label = 'mean of aggr. CPU serving time SVM-MT 4C')\n",
    "plt.errorbar(x_axis, tctswsvm8c, citctswsvm8c, color=\"green\", label = 'mean of aggr. CPU serving time SVM-MT 8C')\n",
    "plt.errorbar(x_axis, tctswsvm16c, citctswsvm16c, color=\"green\", label = 'mean of aggr. CPU serving time SVM-MT 16C')\n",
    "plt.errorbar(x_axis, tctswmvm, citctswmvm, color=\"red\", label = 'mean of aggr. CPU serving time MVM-ST')\n",
    "plt.errorbar(x_axis, trtswbf, citrtswbf, ls=\"dashed\", color=\"blue\", label = 'mean of aggr. thread running time bare-metal')\n",
    "plt.errorbar(x_axis, trtswsvm1c, citrtswsvm1c, ls=\"dashed\", color=\"green\", label = 'mean of aggr. thread running time SVM-MT 1C')\n",
    "plt.errorbar(x_axis, trtswsvm2c, citrtswsvm2c, ls=\"dashed\", color=\"green\", label = 'mean of aggr. thread running time SVM-MT 2C')\n",
    "plt.errorbar(x_axis, trtswsvm4c, citrtswsvm4c, ls=\"dashed\", color=\"green\", label = 'mean of aggr. thread running time SVM-MT 4C')\n",
    "plt.errorbar(x_axis, trtswsvm8c, citrtswsvm8c, ls=\"dashed\", color=\"green\", label = 'mean of aggr. thread running time SVM-MT 8C')\n",
    "plt.errorbar(x_axis, trtswsvm16c, citrtswsvm16c, ls=\"dashed\", color=\"green\", label = 'mean of aggr. thread running time SVM-MT 16c')\n",
    "plt.errorbar(x_axis, trtswmvm, citrtswmvm, ls=\"dashed\", color=\"red\", label = 'mean of aggr. thread running time MVM-ST')\n",
    "#plt.axis([0.9, 16.1, 0, 110000])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=0, borderaxespad=0.)\n",
    "plt.xlabel('concurrent threads/VMs')\n",
    "plt.ylabel('duration (ms)')\n",
    "plt.xticks(x_axis)\n",
    "#plt.savefig('CPUutilisation16procs.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice the waiting time for for bare-metal is greater than the virtualized one while the serving time is slower. This explain part of the reason why the throughput of baremetal is not as good as the virtualised one. But why it spends a lot of time for waiting is till a big question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for throughput per thread/VM\n",
    "nml_tctsrbf = [x/y for x,y in zip(tctsrbf,tctsrbf)]\n",
    "nml_tctswbf = [x/y for x,y in zip(tctswbf,tctswbf)]\n",
    "\n",
    "nml_tctsrsvm1c = [x/y for x,y in zip(tctsrsvm1c,tctsrbf)]\n",
    "nml_tctswsvm1c = [x/y for x,y in zip(tctswsvm1c,tctswbf)]\n",
    "nml_tctsrsvm2c = [x/y for x,y in zip(tctsrsvm2c,tctsrbf)]\n",
    "nml_tctswsvm2c = [x/y for x,y in zip(tctswsvm2c,tctswbf)]\n",
    "nml_tctsrsvm4c = [x/y for x,y in zip(tctsrsvm4c,tctsrbf)]\n",
    "nml_tctswsvm4c = [x/y for x,y in zip(tctswsvm4c,tctswbf)]\n",
    "nml_tctsrsvm8c = [x/y for x,y in zip(tctsrsvm8c,tctsrbf)]\n",
    "nml_tctswsvm8c = [x/y for x,y in zip(tctswsvm8c,tctswbf)]\n",
    "nml_tctsrsvm16c = [x/y for x,y in zip(tctsrsvm16c,tctsrbf)]\n",
    "nml_tctswsvm16c = [x/y for x,y in zip(tctswsvm16c,tctswbf)]\n",
    "\n",
    "nml_tctsrmvm = [x/y for x,y in zip(tctsrmvm,tctsrbf)]\n",
    "nml_tctswmvm = [x/y for x,y in zip(tctswmvm,tctswbf)]\n",
    "\n",
    "\n",
    "# for throughput per thread/VM\n",
    "nml_trtsrbf = [x/y for x,y in zip(trtsrbf,trtsrbf)]\n",
    "nml_trtswbf = [x/y for x,y in zip(trtswbf,trtswbf)]\n",
    "\n",
    "nml_trtsrsvm1c = [x/y for x,y in zip(trtsrsvm1c,trtsrbf)]\n",
    "nml_trtswsvm1c = [x/y for x,y in zip(trtswsvm1c,trtswbf)]\n",
    "nml_trtsrsvm2c = [x/y for x,y in zip(trtsrsvm2c,trtsrbf)]\n",
    "nml_trtswsvm2c = [x/y for x,y in zip(trtswsvm2c,trtswbf)]\n",
    "nml_trtsrsvm4c = [x/y for x,y in zip(trtsrsvm4c,trtsrbf)]\n",
    "nml_trtswsvm4c = [x/y for x,y in zip(trtswsvm4c,trtswbf)]\n",
    "nml_trtsrsvm8c = [x/y for x,y in zip(trtsrsvm8c,trtsrbf)]\n",
    "nml_trtswsvm8c = [x/y for x,y in zip(trtswsvm8c,trtswbf)]\n",
    "nml_trtsrsvm16c = [x/y for x,y in zip(trtsrsvm16c,trtsrbf)]\n",
    "nml_trtswsvm16c = [x/y for x,y in zip(trtswsvm16c,trtswbf)]\n",
    "\n",
    "nml_trtsrmvm = [x/y for x,y in zip(trtsrmvm,trtsrbf)]\n",
    "nml_trtswmvm = [x/y for x,y in zip(trtswmvm,trtswbf)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = 12, 8  # change default size of figures\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(x_axis, nml_tctsrbf, color=\"blue\")\n",
    "plt.plot(x_axis, nml_tctsrsvm1c, color=\"green\")\n",
    "plt.plot(x_axis, nml_tctsrsvm2c, color=\"cyan\")\n",
    "plt.plot(x_axis, nml_tctsrsvm4c, color=\"magenta\")\n",
    "plt.plot(x_axis, nml_tctsrsvm8c, color=\"yellow\")\n",
    "plt.plot(x_axis, nml_tctsrsvm16c, color=\"black\")\n",
    "plt.plot(x_axis, nml_tctsrmvm, color=\"red\")\n",
    "plt.axis([0.9, 16.1, 0, 12])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "plt.xlabel('concurrent threads/VMs')\n",
    "plt.ylabel('ratio of virtualization / bare-metal')\n",
    "plt.xticks(x_axis)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(x_axis, nml_tctswbf, color=\"blue\", label = 'bare-metal')\n",
    "plt.plot(x_axis, nml_tctswsvm1c, color=\"green\", label = 'SVM-MT 1C')\n",
    "plt.plot(x_axis, nml_tctswsvm2c, color=\"cyan\", label = 'SVM-MT 2C')\n",
    "plt.plot(x_axis, nml_tctswsvm4c, color=\"magenta\", label = 'SVM-MT 4C')\n",
    "plt.plot(x_axis, nml_tctswsvm8c, color=\"yellow\", label = 'SVM-MT 8C')\n",
    "plt.plot(x_axis, nml_tctswsvm16c, color=\"black\", label = 'SVM-MT 16C')\n",
    "plt.plot(x_axis, nml_tctswmvm, color=\"red\", label = 'MVM-ST')\n",
    "plt.axis([0.9, 16.1, 0, 12])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('concurrent threads/VMs')\n",
    "#plt.ylabel('ratio of virtualization / bare-metal')\n",
    "plt.xticks(x_axis)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(x_axis, nml_trtsrbf, ls=\"dashed\", color=\"blue\")\n",
    "plt.plot(x_axis, nml_trtsrsvm1c, ls=\"dashed\", color=\"green\")\n",
    "plt.plot(x_axis, nml_trtsrsvm2c, ls=\"dashed\", color=\"cyan\")\n",
    "plt.plot(x_axis, nml_trtsrsvm4c, ls=\"dashed\", color=\"magenta\")\n",
    "plt.plot(x_axis, nml_trtsrsvm8c, ls=\"dashed\", color=\"yellow\")\n",
    "plt.plot(x_axis, nml_trtsrsvm16c, ls=\"dashed\", color=\"black\")\n",
    "plt.plot(x_axis, nml_trtsrmvm, ls=\"dashed\", color=\"red\")\n",
    "plt.axis([0.9, 16.1, 0, 2])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "plt.xlabel('concurrent threads/VMs')\n",
    "plt.ylabel('ratio of virtualization / bare-metal')\n",
    "plt.xticks(x_axis)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(x_axis, nml_trtswbf, ls=\"dashed\", color=\"blue\", label = 'bare-metal')\n",
    "plt.plot(x_axis, nml_trtswsvm1c, ls=\"dashed\", color=\"green\", label = 'SVM-MT 1C')\n",
    "plt.plot(x_axis, nml_trtswsvm2c, ls=\"dashed\", color=\"cyan\", label = 'SVM-MT 2C')\n",
    "plt.plot(x_axis, nml_trtswsvm4c, ls=\"dashed\", color=\"magenta\", label = 'SVM-MT 4C')\n",
    "plt.plot(x_axis, nml_trtswsvm8c, ls=\"dashed\", color=\"yellow\", label = 'SVM-MT 8C')\n",
    "plt.plot(x_axis, nml_trtswsvm16c, ls=\"dashed\", color=\"black\", label = 'SVM-MT 16C')\n",
    "plt.plot(x_axis, nml_trtswmvm, ls=\"dashed\", color=\"red\", label = 'MVM-ST')\n",
    "plt.axis([0.9, 16.1, 0, 2])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('concurrent threads/VMs')\n",
    "#plt.ylabel('ratio of virtualization / bare-metal')\n",
    "plt.xticks(x_axis)\n",
    "\n",
    "#plt.savefig('nml_CPUutilisation16procs.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Switching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtresult = pd.read_csv('data/cpucs16p4gbf12cache_cfq.csv', delimiter=' ', header=None)\n",
    "dtdata = dtresult[range(1,17)]\n",
    "dtindex = dtresult[0]\n",
    "dtdata.index = dtindex.str.replace('T[0-2][0-9]', '',case=False)\n",
    "\n",
    "# Calculate aggregated context switching on each host of each test, then taking the average\n",
    "aggcsin1test = dtdata.T.sum() # Aggregate throughput on each host in one test\n",
    "avgaggcsintestgrouped = aggcsin1test.groupby(level=0)\n",
    "avgaggcsin5tests = avgaggcsintestgrouped.mean() # Then calculate the average of the total CS through 5 tests\n",
    "ciaggcsin5tests = avgaggcsintestgrouped.aggregate(lambda x: np.std(x) / np.sqrt(x.count()) * 1.96) # Confident interval of 95%\n",
    "\n",
    "# Putting average aggregate throughput to vector before plotting\n",
    "aggcssrbf = avgaggcsin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "aggcsswbf = avgaggcsin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "aggcssrsvm1c = avgaggcsin5tests.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "aggcsswsvm1c = avgaggcsin5tests.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "aggcssrsvm2c = avgaggcsin5tests.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "aggcsswsvm2c = avgaggcsin5tests.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "aggcssrsvm4c = avgaggcsin5tests.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "aggcsswsvm4c = avgaggcsin5tests.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "aggcssrsvm8c = avgaggcsin5tests.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "aggcsswsvm8c = avgaggcsin5tests.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "aggcssrsvm16c = avgaggcsin5tests.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "aggcsswsvm16c = avgaggcsin5tests.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "aggcssrmvm = avgaggcsin5tests.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "aggcsswmvm = avgaggcsin5tests.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n",
    "\n",
    "# Confidence Interval\n",
    "ciaggcssrbf = ciaggcsin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "ciaggcsswbf = ciaggcsin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "ciaggcssrsvm1c = ciaggcsin5tests.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "ciaggcsswsvm1c = ciaggcsin5tests.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "ciaggcssrsvm2c = ciaggcsin5tests.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "ciaggcsswsvm2c = ciaggcsin5tests.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "ciaggcssrsvm4c = ciaggcsin5tests.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "ciaggcsswsvm4c = ciaggcsin5tests.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "ciaggcssrsvm8c = ciaggcsin5tests.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "ciaggcsswsvm8c = ciaggcsin5tests.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "ciaggcssrsvm16c = ciaggcsin5tests.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "ciaggcsswsvm16c = ciaggcsin5tests.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "ciaggcssrmvm = ciaggcsin5tests.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "ciaggcsswmvm = ciaggcsin5tests.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 12, 6  # change default size of figures\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.errorbar(x_axis, aggcssrbf, ciaggcssrbf)\n",
    "plt.errorbar(x_axis, aggcssrsvm1c, ciaggcssrsvm1c)\n",
    "plt.errorbar(x_axis, aggcssrsvm2c, ciaggcssrsvm2c)\n",
    "plt.errorbar(x_axis, aggcssrsvm4c, ciaggcssrsvm4c)\n",
    "plt.errorbar(x_axis, aggcssrsvm8c, ciaggcssrsvm8c)\n",
    "plt.errorbar(x_axis, aggcssrsvm16c, ciaggcssrsvm16c)\n",
    "plt.errorbar(x_axis, aggcssrmvm, ciaggcssrmvm)\n",
    "#plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('unit')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.errorbar(x_axis, aggcsswbf, ciaggcsswbf, label = 'bare-metal')\n",
    "plt.errorbar(x_axis, aggcsswsvm1c, ciaggcsswsvm1c, label = 'SVM-MT 1 core')\n",
    "plt.errorbar(x_axis, aggcsswsvm2c, ciaggcsswsvm2c, label = 'SVM-MT 2 cores')\n",
    "plt.errorbar(x_axis, aggcsswsvm4c, ciaggcsswsvm4c, label = 'SVM-MT 4 cores')\n",
    "plt.errorbar(x_axis, aggcsswsvm8c, ciaggcsswsvm8c, label = 'SVM-MT 8 cores')\n",
    "plt.errorbar(x_axis, aggcsswsvm16c, ciaggcsswsvm16c, label = 'SVM-MT 16 cores')\n",
    "plt.errorbar(x_axis, aggcsswmvm, ciaggcsswmvm, label = 'MVM-ST 1 core')\n",
    "#plt.axis([0.9, 16.1, 0, 110])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('unit')\n",
    "\n",
    "#plt.savefig('throughput16procs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 5, 5  # change default size of figures\n",
    "\n",
    "plt.errorbar(x_axis, aggcssrbf, ciaggcssrbf)\n",
    "plt.errorbar(x_axis, aggcssrsvm1c, ciaggcssrsvm1c)\n",
    "plt.errorbar(x_axis, aggcssrsvm2c, ciaggcssrsvm2c)\n",
    "plt.errorbar(x_axis, aggcssrsvm4c, ciaggcssrsvm4c)\n",
    "plt.errorbar(x_axis, aggcssrsvm8c, ciaggcssrsvm8c)\n",
    "plt.errorbar(x_axis, aggcssrsvm16c, ciaggcssrsvm16c)\n",
    "plt.errorbar(x_axis, aggcssrmvm, ciaggcssrmvm)\n",
    "plt.axis([0.9, 16.1, 0, 12000000])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('context switch')\n",
    "plt.savefig('contextswitching_cfq_read.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 5, 5  # change default size of figures\n",
    "plt.errorbar(x_axis, aggcsswbf, ciaggcsswbf, label = 'bare-metal')\n",
    "plt.errorbar(x_axis, aggcsswsvm1c, ciaggcsswsvm1c, label = 'SVM-MT 1 core')\n",
    "plt.errorbar(x_axis, aggcsswsvm2c, ciaggcsswsvm2c, label = 'SVM-MT 2 cores')\n",
    "plt.errorbar(x_axis, aggcsswsvm4c, ciaggcsswsvm4c, label = 'SVM-MT 4 cores')\n",
    "plt.errorbar(x_axis, aggcsswsvm8c, ciaggcsswsvm8c, label = 'SVM-MT 8 cores')\n",
    "plt.errorbar(x_axis, aggcsswsvm16c, ciaggcsswsvm16c, label = 'SVM-MT 16 cores')\n",
    "plt.errorbar(x_axis, aggcsswmvm, ciaggcsswmvm, label = 'MVM-ST 1 core')\n",
    "plt.axis([0.9, 16.1, 0, 12000000])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('context switches')\n",
    "\n",
    "plt.savefig('contextswitching_cfq_write.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtresult = pd.read_csv('data/iops16p4gbf12cache_cfq.csv', delimiter=' ', header=None)\n",
    "dtdata = dtresult[range(1,17)]\n",
    "dtindex = dtresult[0]\n",
    "dtdata.index = dtindex.str.replace('T[0-2][0-9]', '',case=False)\n",
    "\n",
    "# Calculate aggregated context switching on each host of each test, then taking the average\n",
    "aggiopsin1test = dtdata.T.sum() # Aggregate throughput on each host in one test\n",
    "avgaggiopsintestgrouped = aggiopsin1test.groupby(level=0)\n",
    "avgaggiopsin5tests = avgaggiopsintestgrouped.mean() # Then calculate the average of the total CS through 5 tests\n",
    "ciaggiopsin5tests = avgaggiopsintestgrouped.aggregate(lambda x: np.std(x) / np.sqrt(x.count()) * 1.96) # Confident interval of 95%\n",
    "\n",
    "# Putting average aggregate throughput to vector before plotting\n",
    "aggiopssrbf = avgaggiopsin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "aggiopsswbf = avgaggiopsin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "aggiopssrsvm1c = avgaggiopsin5tests.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "aggiopsswsvm1c = avgaggiopsin5tests.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "aggiopssrsvm2c = avgaggiopsin5tests.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "aggiopsswsvm2c = avgaggiopsin5tests.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "aggiopssrsvm4c = avgaggiopsin5tests.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "aggiopsswsvm4c = avgaggiopsin5tests.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "aggiopssrsvm8c = avgaggiopsin5tests.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "aggiopsswsvm8c = avgaggiopsin5tests.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "aggiopssrsvm16c = avgaggiopsin5tests.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "aggiopsswsvm16c = avgaggiopsin5tests.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "aggiopssrmvm = avgaggiopsin5tests.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "aggiopsswmvm = avgaggiopsin5tests.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n",
    "\n",
    "# Confidence Interval\n",
    "ciaggiopssrbf = ciaggiopsin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "ciaggiopsswbf = ciaggiopsin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "ciaggiopssrsvm1c = ciaggiopsin5tests.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "ciaggiopsswsvm1c = ciaggiopsin5tests.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "ciaggiopssrsvm2c = ciaggiopsin5tests.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "ciaggiopsswsvm2c = ciaggiopsin5tests.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "ciaggiopssrsvm4c = ciaggiopsin5tests.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "ciaggiopsswsvm4c = ciaggiopsin5tests.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "ciaggiopssrsvm8c = ciaggiopsin5tests.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "ciaggiopsswsvm8c = ciaggiopsin5tests.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "ciaggiopssrsvm16c = ciaggiopsin5tests.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "ciaggiopsswsvm16c = ciaggiopsin5tests.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "ciaggiopssrmvm = ciaggiopsin5tests.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "ciaggiopsswmvm = ciaggiopsin5tests.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n",
    "\n",
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 12, 6  # change default size of figures\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.errorbar(x_axis, aggiopssrbf, ciaggiopssrbf)\n",
    "plt.errorbar(x_axis, aggiopssrsvm1c, ciaggiopssrsvm1c)\n",
    "plt.errorbar(x_axis, aggiopssrsvm2c, ciaggiopssrsvm2c)\n",
    "plt.errorbar(x_axis, aggiopssrsvm4c, ciaggiopssrsvm4c)\n",
    "plt.errorbar(x_axis, aggiopssrsvm8c, ciaggiopssrsvm8c)\n",
    "plt.errorbar(x_axis, aggiopssrsvm16c, ciaggiopssrsvm16c)\n",
    "plt.errorbar(x_axis, aggiopssrmvm, ciaggiopssrmvm)\n",
    "plt.axis([0.9, 16.1, 0, 28000])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('IO-rqt/s')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.errorbar(x_axis, aggiopsswbf, ciaggiopsswbf, label = 'bare-metal')\n",
    "plt.errorbar(x_axis, aggiopsswsvm1c, ciaggiopsswsvm1c, label = 'SVM-MT 1 core')\n",
    "plt.errorbar(x_axis, aggiopsswsvm2c, ciaggiopsswsvm2c, label = 'SVM-MT 2 cores')\n",
    "plt.errorbar(x_axis, aggiopsswsvm4c, ciaggiopsswsvm4c, label = 'SVM-MT 4 cores')\n",
    "plt.errorbar(x_axis, aggiopsswsvm8c, ciaggiopsswsvm8c, label = 'SVM-MT 8 cores')\n",
    "plt.errorbar(x_axis, aggiopsswsvm16c, ciaggiopsswsvm16c, label = 'SVM-MT 16 cores')\n",
    "plt.errorbar(x_axis, aggiopsswmvm, ciaggiopsswmvm, label = 'MVM-ST 1 core')\n",
    "plt.axis([0.9, 16.1, 0, 28000])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('IO_rqt/s')\n",
    "\n",
    "#plt.savefig('throughput16procs.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtresult = pd.read_csv('data/lat16p4gbf12cache_cfq.csv', delimiter=' ', header=None)\n",
    "dtdata = dtresult[range(1,17)]\n",
    "dtindex = dtresult[0]\n",
    "dtdata.index = dtindex.str.replace('T[0-2][0-9]', '',case=False)\n",
    "\n",
    "# mean of all member in diferent group = mean of group mean when all groups have same number of member\n",
    "# Calculate aggregated context switching on each host of each test, then taking the average\n",
    "latin1test = dtdata.T.mean()\n",
    "avglatintestgrouped = latin1test.groupby(level=0)\n",
    "avglatin5tests = avglatintestgrouped.aggregate(np.mean) # Then calculate the average of the total CS through 5 tests\n",
    "cilatin5tests = avglatintestgrouped.aggregate(lambda x: np.std(x) / np.sqrt(x.count()) * 1.96) # Confident interval of 95%\n",
    "\n",
    "# Putting average aggregate throughput to vector before plotting\n",
    "latsrbf = avglatin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "latswbf = avglatin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "latsrsvm1c = avglatin5tests.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "latswsvm1c = avglatin5tests.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "latsrsvm2c = avglatin5tests.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "latswsvm2c = avglatin5tests.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "latsrsvm4c = avglatin5tests.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "latswsvm4c = avglatin5tests.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "latsrsvm8c = avglatin5tests.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "latswsvm8c = avglatin5tests.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "latsrsvm16c = avglatin5tests.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "latswsvm16c = avglatin5tests.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "latsrmvm = avglatin5tests.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "latswmvm = avglatin5tests.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n",
    "\n",
    "# Confidence Interval\n",
    "cilatsrbf = cilatin5tests.filter(regex=\"^SRBF32CTH\").tolist()\n",
    "cilatswbf = cilatin5tests.filter(regex=\"^SWBF32CTH\").tolist()\n",
    "cilatsrsvm1c = cilatin5tests.filter(regex=\"^SR01VM01CTH\").tolist()\n",
    "cilatswsvm1c = cilatin5tests.filter(regex=\"^SW01VM01CTH\").tolist()\n",
    "cilatsrsvm2c = cilatin5tests.filter(regex=\"^SR01VM02CTH\").tolist()\n",
    "cilatswsvm2c = cilatin5tests.filter(regex=\"^SW01VM02CTH\").tolist()\n",
    "cilatsrsvm4c = cilatin5tests.filter(regex=\"^SR01VM04CTH\").tolist()\n",
    "cilatswsvm4c = cilatin5tests.filter(regex=\"^SW01VM04CTH\").tolist()\n",
    "cilatsrsvm8c = cilatin5tests.filter(regex=\"^SR01VM08CTH\").tolist()\n",
    "cilatswsvm8c = cilatin5tests.filter(regex=\"^SW01VM08CTH\").tolist()\n",
    "cilatsrsvm16c = cilatin5tests.filter(regex=\"^SR01VM16CTH\").tolist()\n",
    "cilatswsvm16c = cilatin5tests.filter(regex=\"^SW01VM16CTH\").tolist()\n",
    "cilatsrmvm = cilatin5tests.filter(regex=\"^SR[0-9]+VM01CTH01\").tolist()\n",
    "cilatswmvm = cilatin5tests.filter(regex=\"^SW[0-9]+VM01CTH01\").tolist()\n",
    "\n",
    "x_axis = [1,2,4,8,16]\n",
    "pylab.rcParams['figure.figsize'] = 12, 6  # change default size of figures\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.errorbar(x_axis, latsrbf, cilatsrbf)\n",
    "plt.errorbar(x_axis, latsrsvm1c, cilatsrsvm1c)\n",
    "plt.errorbar(x_axis, latsrsvm2c, cilatsrsvm2c)\n",
    "plt.errorbar(x_axis, latsrsvm4c, cilatsrsvm4c)\n",
    "plt.errorbar(x_axis, latsrsvm8c, cilatsrsvm8c)\n",
    "plt.errorbar(x_axis, latsrsvm16c, cilatsrsvm16c)\n",
    "plt.errorbar(x_axis, latsrmvm, cilatsrmvm)\n",
    "#plt.axis([0.9, 16.1, 0, 28000])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Read')\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('us')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.errorbar(x_axis, latswbf, cilatswbf, label = 'bare-metal')\n",
    "plt.errorbar(x_axis, latswsvm1c, cilatswsvm1c, label = 'SVM-MT 1 core')\n",
    "plt.errorbar(x_axis, latswsvm2c, cilatswsvm2c, label = 'SVM-MT 2 cores')\n",
    "plt.errorbar(x_axis, latswsvm4c, cilatswsvm4c, label = 'SVM-MT 4 cores')\n",
    "plt.errorbar(x_axis, latswsvm8c, cilatswsvm8c, label = 'SVM-MT 8 cores')\n",
    "plt.errorbar(x_axis, latswsvm16c, cilatswsvm16c, label = 'SVM-MT 16 cores')\n",
    "plt.errorbar(x_axis, latswmvm, cilatswmvm, label = 'MVM-ST 1 core')\n",
    "#plt.axis([0.9, 16.1, 0, 28000])\n",
    "plt.grid(True)\n",
    "plt.title('Sequential Write')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "pylab.xlabel('concurrent threads/VMs')\n",
    "pylab.ylabel('us')\n",
    "\n",
    "#plt.savefig('throughput16procs.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disk Utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disk was used by 100% in all cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
