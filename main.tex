\documentclass{acmsig}

\usepackage[color=yellow,obeyFinal]{todonotes}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\graphicspath{ {notebook/} }
\usepackage{epstopdf}
\usepackage{float}
\usepackage{varioref}
\usepackage{mathtools}
\usepackage{xspace}
%\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{balance}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{paralist}
\usepackage{cite}
\usepackage{color}

\usepackage{xcolor}
\definecolor{dark-red}{rgb}{0.4,0.15,0.15}
\definecolor{dark-blue}{rgb}{0.15,0.15,0.4}
\definecolor{medium-blue}{rgb}{0,0,0.5}
\hypersetup{
  colorlinks, linkcolor={dark-red},
  citecolor={dark-blue}, urlcolor={medium-blue}
}

\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\etc}{\textit{etc.}\xspace}
\newcommand{\vs}{\textit{vs.}\xspace}
\newcommand{\keyval}{$\langle\text{key}, \text{value}\rangle$\xspace}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\newcommand{\noteby}[2]{\todo[inline]{#2\hspace*{\fill}\mbox{ --#1}}}

\lstset{frame=tb,
  language=SQL,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{blue},
  stringstyle=\color{mauve},
  keywordstyle=\color{blue},
  commentstyle=\color{Brown},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{The Impacts of Virtualization\\ on Big Data Application's Workload}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\numberofauthors{4}
\author{
\alignauthor
Ha Son Hai\\
       \affaddr{Orange/EURECOM}\\
       \email{sonhai.ha@orange.com}
\alignauthor
Daniele Venzano\\
       \affaddr{EURECOM}\\
       \email{venzano@eurecom.fr}
\and
\alignauthor
Patrick Brown\\
       \affaddr{Orange}\\
       \email{patrick.brown@orange.fr}
\alignauthor
Pietro Michiardi
       \affaddr{EURECOM}\\
       \email{michiard@eurecom.fr}
}

% make the title area
\maketitle


\begin{abstract}
Bla bla bla bla bla
Keep in mind, this paper is talking about the IMPACTS, and OVERHEAD of VIRTUALIZATION is included in the IMPACTS. -> Make the paper more general and easily for us to integrate fairness metric into the paper.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Need better motivation for this part
The adoption of virtualization brings many benefits to the technology community. It gives ways to high flexibility, high availability and high utilisation of the hardware. But it also brings the disadvantages by adding complexity to the management and overhead to the performance. On the other hand, in 2014, IDG published a study in which they found that more than 70\% of enterprise organizations have either deployed or are planning to deploy big data-related projects and programs \cite{idg}. Bringing big data application to the cloud is surely unavoidable in the future. We are seeing more and more Hadoop \cite{apachehadoop} or Spark \cite{apachespark} cluster operating on the cloud. The marriage of big data application and virtualization is already happened and continued to grow more and more greatly than ever. However, when mentioning about the performance of Big Data application on the cloud, people just walk away with the common thought that virtualization bringing bad performance to big data application [?-?] because of the added virtualization layer. Yet we wonder, if it is really bad, how bad it is? to the most of our knowledge, there is no or very few works that directly studies the impacts of virtualization on the big data application performance.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/cluster_snapshot.png}
    \caption{\textit{An example snapshot of a running virtualised MapReduce cluster}}
    \label{cluster_snapshot}
\end{figure}

% Need a glue paragraph here

Measuring the performance of a distributed application is hard, and it is even harder when that application is running on virtualized platform which involving layers and layers of different technologies. Yet the measurement is necessary to have a proper understanding on performance of Big Data applications in the virtualized environment. Figure \ref{cluster_snapshot} is an abstract snapshot of a small big data application operating in virtualized environment at a random time. There are n physical servers which host multiple Virtual Machines on each of them. These VMs were set-up to run a big data application which involved many parallel tasks executing in contiguous stages. Those tasks were spawn as threads and concurrently competing for the same resources at each physical host such as disk storage, memory, and CPU,... When looking at the whole picture, we see that the performance of the whole cluster at each stage is depended on the performance of every one of its building block: one single physical host. We would like to see the different in storage I/O performance between virtualization platform and bare-metal system with the big data workloads so we designed a synthesis big data workload and run our measurements with two scenarios for virtualization and one scenario for bare-metal:
\begin{itemize}
\item Single VM with multiple threads (SVM-MT): this scenario (figure \ref{fig:svm-mt}) represents the case that the host has only one ``fat`` VM but this VM runs many parallel tasks at the same time. This SVM-MT scenario focuses on the guest OS scheduler since it has to switch between active threads. We consider 5 different flavors (sizes) of VM for this scenario to see how the VM's size affects on the performance.
\item Multiple VMs with single thread (MVM-ST): this scenario (figure \ref{fig:mvm-st}) represents the case that we have many ``slim`` VM and these VMs run only one task at a time inside its guest OS. They are colocated on the same host and they blindly competing for the same I/O resources. This scenario focuses on the hypervisor scheduler since the host has to switch between active VMs.
\item Bare-metal (BM): the bare-metal system is used as the baseline to make the comparison (figure \ref{fig:bm}). In case the big data application runs on a bare-metal environment, many parallel tasks running at the same time requesting for IO resources. In short, it is similar to the case of SVM-MT without the virtualized layer.
\end{itemize}
In this paper, we establish our focus only on the performance of shared storage I/O since it was thought to be one of the main bottlenecks of big data application [5]. More specific, we are especially interesting in the performance of big data workload - mainly sequential read and sequential write in parallel issued from concurrent tasks - on the shared storage I/O. We consider I/O throughput as the metric to compare the performance between bare-metal and virtualized platform. We also keep our observation on the fairness of both platforms. The throughput metric expresses how well the system feed data to the threads while fairness keeps threads away from data starvation. Unfairness in treatment between processes creates a heterogeneous environment in which one process can becomes a straggler \cite{matei08} and heighten job completion time which in the end reduce the responsiveness of the application. We also consider CPU utilization, disk utilization, and the number of context switching in the experiments to "pinpoint" the components which affected most on the performance of shared I/O.

% This part talk about the expectation and the result of the paper, need to write it better
Before the study, our expectation was set on the conclusion of the overhead of virtualization on big data application workload. One part of our study repeats the findings of previous studies which show that, with proper tuning, virtualization can only reach maximum 80-90\% of bare metal performance due to the overhead that the added virtualization layer brought up. Previous studies reveal the performance of the system consider little about the different in level of concurrency and they also did not consider the big-data-application-like workload. Our studies reveal the big picture when there are different number of active threads concurrently competing for I/O. We consider three difference schedulers on the host: Noop, Deadline,and Scheduler and show the different behaviors of the baremetal and the virtualized platform in IO performance when using one scheduler. It is unexpected that using Deadline and Noop scheduler, Bare-metal can get less throughput than the VM that it hosts when level of concurrency is high. For CFQ scheduler, Bare-metal have good throughput and change little when there are many active threads executing but the VMs can get only 40\% of the total throughput that a Bare-metal can get. We see that both virtualized and bare-metal platform achieve high fairness index. There is no or very little different between bare-metal and virtualized platform in fairness index.

In the next sections, we introduce to our readers the methodologies in detail. We also present the findings and explanation for the result that we got. Finally, we summarize our work at the last section and suggest the future approaches to give more meaningful results to the study.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and Related Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
  \item introduce some related works on the performance of virtualization: work from IBM on performance of KVM, work from a guy discussing the relevant of IO schedulers to virtualization system.
  \item introduce some related works on measuring performance of data intensive application
 \item then mention about the combined of both, what we are different from previous works, what are our interesting points.
\end{itemize}

In the Big Data world, data volume is too big that people avoiding moving it. With the belief that network is the bottleneck of the distributed systems, the developers try to work around by moving the execution/programs to the data but not the other way round (like Hadoop or Spark). By such method of task distributing, the burden was shifted to storage I/O (disk and memory) performance. A typical example is Hadoop with data-locality awareness. It reported achieving greater throughput when utilising data locality[?-?]. But then it makes disk I/O become the bottle-neck. Data was read from disk, intermediate data was stored on disk, and results were written to disk. Spark has done a step further, when they leverage memory to use together with the disk, Spark reported the performance improvement up to severalfold in comparing to Hadoop with data locality awareness [?]. However, memory and cache sharing was not developed well enough to also provide fault tolerant to other big data applications but Spark. Then people use data compressing techniques to trade CPU resource for storage I/O performance. But then CPU becomes the bottle-neck [?]. The simplest model is the most efficient model. That is why many data intensive applications are still greatly depended on the hard drive performance since the developers and the administrators do not want to complicate a distributed system.

% Summary disk_schedulers_passe
%
% Summary works on measurement of virtualization overhead, indicate they are not focus on big data workload, most of them are white paper.

% What we different from other works? we focus on Big Data wordload, we make a simulated workload represented Hadoop and run the measurement on that workload.

% cite: Locality-Aware Reduce Task Scheduling for MapReduce - Mohammed Hammoud, CloudCom
% cite: Purlieus: Locality-aware Resource Allocation for MapReduce in a Cloud - Balaji Balanisamy
% cite: Locality-Aware Dynamic VM Reconfiguration on MapReduce Clouds - Jongse Park

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem statement}

Given the facts that virtualization improves resource utilization and enable elasticity and multi-tenancy. Yet it also brings the disadvantages. On the architecture viewpoint, virtualization brings the ``Seeing double`` problem. There may be two file systems. The guest file system and the host file system which holds the image file. There may be two volume managers. Logical volume manager at the guest and logical volume manager at the host. There may be two page caches. Both guest and host can buffer pages from a file. There may be two I/O schedulers. The guest will reorder or delay I/O and the host will too. All of this add latencies and bottleneck at different places to IO performance.

When running data intensive application in virtualized environment, the situation is worse. There is not only ``seeing double`` problem, but also the competing between VMs sharing the same resources. Many VMs can be located on the same hard drive. They can also use the same network card to communicate with other VMs. In order to utilise data locality characteristic, the VM were setup to operate on local hard drives and it creates a situation in which many concurrently processes competing for IO resources when the distributed application processing massive data input and output.

Is it worthy to trade performance for virtualization benefits? Even we are looking at the application level to measure the impact of virtualization on the big data technology. we still need a proper understanding on what happens without the application because the application is very complex. It is a large distributed system that hides a lot of details so whenever you see the result, the problem is that it's very hard to explain what happened if you only look from the perspective of the application. That was the reason why we simplify the thing. Let try to somehow emulate the traditional workload generated by application such as Hadoop MapReduce. We designed a synthesis big data workload to mimic that typical I/O ``pressure'' which comes from data-intensive applications and do the experiments to compare and explore the I/O performance of bare-metal and virtualized platform. Given the context that many big data application are still greatly depended on disk I/O performance, we wonder how virtualization can affect on the I/O performance dedicated to big data application. The question was what happens when you have the virtualization layer that hide in the middle between your application and IO call. And that why we have the measurement of bare-metal with several different configuration of the hard drive setting and CPU setting. And then we have result that look at different flavors of CPU of virtual machine with different size. And also the number of concurrent threads. These are the useful foundation for understanding the of big data application which has similar workload.

%Guide from P.Pietro: Even we looking at the application level. Measure the impact of virtualization on the big data technology. we still need a proper understanding on what happens without the application because the application is very complex. it's a large distributed system that hides a lot of details so whenever you see the result, the problem is that it's very hard to explain what happened if you only look from the perspective of Hadoop or Spark. That was the reason why we simplify the thing. Let try to somehow emulate  what is the traditional workload generated by application such as Hadoop MapReduce. The question was what happens when you have the virtualization layer that hide in the middle between your application and IO call. And that why we have the measurement of bare-metal with several different configuration of the hard drive setting and CPU setting. We show that more or less the performance did not change much. So bare metal is really stable. And then we have result that look at different flavors of CPU of virtual machine with different size. And also the number of concurrent threads. These are the useful foundation for understanding the next result that you will obtain by running some sample of map reduce application which have similar workload. State what your goal are. You establish a measurement methodology where you explain your experimental setup, your tools, metrics, why they're important. And there you explain the statistical perspective of experiments. Then to validate your initial claim once the paper is done in the introduction (for example "we see that Hadoop on virtualization is suck"). You anticipate the result.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The methodology}

\subsection{The system under measurement}

Our system runs on a dual hexa-core Xeon E5-2650L (with hyperthreading enabled) server clocked at 1.8GHz, with 128GB of RAM, ten 1TB disks. Disks are all the same make and model, across the hosts: SEAGATE ST91000640SS. The hardware RAID controller is configured with RAID 0 for each disk with write-back cache enabled. For easy interpreting the results, only one hard drive is use to host the VMs running the experiments. We also avoid  CPU and memory over-commit to make sure that our results were not affected by memory and CPU bottlenecks. The host machine runs the latest long term support Ubuntu 14.04 distribution, updated with the most recent patches. QEMU is used as the virtualizer to executing the guest code directly on the host CPU using the KVM kernel module in Linux. All energy saving settings in the BIOS are disabled, since they cause severe performance penalties. It uses the KVM hypervisor, with \texttt{virtio} and \texttt{vhost\_net} acceleration modules enabled. Virtualization support in the CPUs is enabled (VMX) and KVM uses it automatically. The hypervisor is configured to use LVM for VM storage. DevStack, which is an collection of scripts to quickly create an OpenStack development environment, was used to manage the deployment of VMs on the host. I/O scheduler is disable in guests' OS (in fact the guests' OS uses noop scheduler, a very simple FIFO scheduler), and we switch between three different I/O schedulers of the host: Noop, Deadline, and Completed Fair Queueing (CFQ) to run experiments.
\subsection{Overview on the components involved}

  \subsubsection{The path of a I/O request}

  \begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/iopath.png}
    \caption{\textit{The path of the I/O request }}
    \label{iopath}
  \end{figure}
 
  When a VM is created, QEMU (the userspace process that performs most device emulation and controls the guest) creates a process and forks a number of vCPU threads associating with same number of virtual cpu that the VM was allocated. These dedicated vcpu threads use the kvm.ko module in the kernel to execute guest code. Besides, the ``iothread`` thread is created for handling IO for this virtual machine. There is an I/O thread that runs a select loop to handle events. Only one thread may be executing QEMU code at any given time.

  Application and guest kernel work similar to bare metal. Guest talks to QEMU via emulated hardware. QEMU performs I/O to an image file on behalf of the guest. Host kernel treats guest I/O like any user space application. QEMU presents emulated storage interfaces to the guest. Virtio is a paravirtualized storage interface, delivers the best performance. One virtio-blk PCI adapter per block device.
  
  At the guest or the host, applications submit I/O requests via a kernel system call, that converts them into a data structure called a block IO. Each block IO contains information such as IO address, IO size, IO modality (read or write) or IO type (synchronous/asynchronous). It is then transferred to either libaio for asynchronous IOs or directly to the block layer for synchronous IO that submit it to the block layer. Once an IO request is submitted, the corresponding block IO is buffered in the staging area, which is implemented as a request queue. Once a request is in the staging area, the block layer may perform IO scheduling and adjust accounting information before scheduling IO submissions to the appropriate storage device driver. Linux block layer supports pluggable IO schedulers: noop (no scheduling), deadline-based scheduling, and CFQ that can all operate on IO within this staging area. The block layer also provides a mechanism for dealing with IO completions: each time an IO completes within the device driver, this driver calls up the stack to the generic completion function in the block layer. In turn the block layer then calls up to an IO completion function in the libaio library, or returns from the synchronous read or write system call, which provides the IO completion signal to the application.
  
  Between the guest and the host, The I/O requests were delivered thanks to the simulated device. Guest I/O request was issued to the virtual device. This device then fills in the request descriptors which is then written to virtio-blk virtqueue notify register. QEMU issues I/O request on behalf of the guest to just like any other application. After the data was arrive at QEMU buffer. QEMU fills in request footer and injects completion interupt to guest kernel. Guest receives the interrupt and executes handler. It then reads data from the buffer.

  \subsubsection{Linux IO Schedulers}

  Application and guest kernel work similar to bare metal. Guest talks to QEMU via emulated hardware. QEMU performs I/O to an image file on behalf of the guest. Host kernel treats guest I/O like any userspace application. Therefore, the I/O performance of the guests is depended greatly on the host's I/O schedulers. Understanding how I/O schedulers works, how the kernel seeing a guest as an application, how the guest runs on the host can help to pinpoint the overheads of the virtualized platform.

    \paragraph{CFQ scheduler}
    The CFQ I/O scheduler is one of the most sophisticated I/O schedulers. The CFQ I/O scheduler divides the available I/O bandwidth among all of the processes that issue I/O requests. The CFQ I/O scheduler maintains per-process queues for synchronous I/O requests. The maximum number of per-process queues that the CFQ I/O scheduler maintains for synchronous I/O requests is 64. The CFQ I/O scheduler batches together asynchronous requests from all processes based on the priorities of the processes. For example, the CFQ I/O scheduler maintains one process queue for all asynchronous requests from processes with the idle scheduling priority.
    During each cycle, the CFQ I/O scheduler moves one request from each queue to the dispatch queue. After the CFQ I/O scheduler moves a request from each queue, it repeats the process and removes another request from each queue. After the CFQ I/O scheduler moves requests to the dispatch queue, it sorts the requests to minimize disk seeks and then services the requests accordingly. The CFQ I/O scheduler provides each queue with time to access the disk. The length of time depends on the scheduling priority of the process.
    CFQ places synchronous requests submitted by processes into a number of per-process queues and then allocates timeslices for each of the queues to access the disk. The length of the time slice and the number of requests a queue is allowed to submit depends on the I/O priority of the given process. Asynchronous requests for all processes are batched together in fewer queues, one per priority. While CFQ does not do explicit anticipatory I/O scheduling, it achieves the same effect of having good aggregate throughput for the system as a whole, by allowing a process queue to idle at the end of synchronous I/O thereby "anticipating" further close I/O from that process. It can be considered a natural extension of granting I/O time slices to a process.

    \paragraph{Deadline scheduler}
    The Deadline I/O scheduler is one of the most sophisticated I/O schedulers. The Deadline I/O scheduler dispatches I/O requests based on the length of time that the I/O requests are in the queues. Therefore, the Deadline I/O scheduler guarantees a start service time for each I/O request. The Deadline I/O scheduler maintains the deadline queues by the expiration times, or deadlines, of the I/O requests and the other queues by the positions of the requests on the disks, or sector numbers. Each set of queues, deadline queues and other queues, includes read queues and write queues. The read queues contain read requests. Because processes often block on read operations, the Deadline I/O scheduler prioritizes read requests higher than write requests and assigns read requests shorter expiration times than write requests. The write queues contain write requests.

    Based on prioritization and expiration times, the Deadline I/O scheduler determines which request from which queue to dispatch. The Deadline I/O scheduler checks to see if the first request in the deadline queue expired. If the first request in the deadline queue expired, then the Deadline I/O scheduler services the first request in the deadline queue immediately. Otherwise it services the first request from the sorted queue. To improve disk efficiency, the Deadline I/O scheduler not only services one request but also services a batch of requests near the disk location of the request that it has just taken at the top of the queue.

    \paragraph{NOOP scheduler}
    The NOOP scheduler is the simplest I/O scheduler for the Linux kernel. It inserts all incoming I/O requests into a simple FIFO queue and merges them. This scheduler is useful when it has been determined that the host should not attempt to re-order requests based on the sector numbers contained therein. In other words, the scheduler assumes that the host is unaware of how to productively re-order requests. Since I/O requests are potentially re-scheduled at the lower level, resequencing I/O request at the host level can create a situation where CPU time on the host is being spent on operations that will just be undone when they reach the lower level, increasing latency/decreasing throughput for no productive reason.

  \subsubsection{The caching system}
  Figure \ref{cache_system} is the view of our cache system when there is VM running. QEMU uses the host's Page Cache to cache requested data there. Page Cache is maintained by the operating system to improve I/O performance. Write operations are considered completed after the data has been copied to the page cache and Read operations can be satisfied from the page cache if the data requested is in the cache. In the KVM environment, both the host and guest operating systems can maintain their own page caches, resulting in two copies of data in memory. In general, it is better to bypass at least one of these page caches. If the application running in the guest is using direct I/O operations, then the guest cache would be bypassed. If the guest is set up with no caching, then the host page cache is bypassed, effectively turning all I/O operations from the guest into direct I/O operations on the host.

  In general, with the disk write cache enabled, the write performance is improved significantly, but the data integrity and protection in case of power loss can be ensured only if the applications and the storage stack transfer the cache data to permanent storage correctly. However, if the disk write cache is disabled, the write performance might suffer while the risk of data loss in a power failure is lessened.

  \begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/cachesystem.png}
    \caption{\textit{The cache system of the host with running VMs }}
    \label{cache_system}
\end{figure}

  QEMU executes as a device simulator for guest OS. It is implemented with 4 types of cache techniques. The cache space used is the host's page cache. By loading the VM image with 4 different combinations of the control flags: O\_DIRECT and O\_SYNC, QEMU effectively provides 4 different cache techniques to the guest.
  
\begin{itemize}
 \item \textbf{none}: host does not do cache, guest disk cache is writeback. This mode causes QEMU to interact with the disk image file or block device with O\_DIRECT semantics, so the host page cache is bypassed and I/O happens directly between the QEMU userspace buffers and the storage device. Because the actual storage device may report a write as completed when placed in its write queue only, the guest's virtual storage adapter is informed that there is a writeback cache, so the guest would be expected to send down flush commands as needed to manage data integrity. Equivalent to direct access to the host's disk.
 \item \textbf{writethrough}: host does read cache, guest disk cache mode is writethrough. Writethrough make a fsync for each write. So it's the more secure cache mode, you can't loose data. It's also the slower. This mode causes QEMU to interact with the disk image file or block device with O\_SYNC semantics, where writes are reported as completed only when the data has been committed to the storage device. The host page cache is used in what can be termed a writethrough caching mode. The guest's virtual storage adapter is informed that there is no writeback cache, so the guest would not need to send down flush commands to manage data integrity. The storage behaves as if there is a writethrough cache.
 \item \textbf{writeback}: host does read/write cache. Guest disk cache mode is writeback. This mode causes QEMU to interact with the disk image file or block device with neither O\_SYNC nor O\_DIRECT semantics, so the host page cache is used and writes are reported to the guest as completed when placed in the host page cache, and the normal page cache management will handle commitment to the storage device. Additionally, the guest's virtual storage adapter is informed of the writeback cache, so the guest would be expected to send down flush commands as needed to manage data integrity. Analogous to a raid controller with RAM cache.
 \item \textbf{directsync}: host does not do cache. guest disk cache mode is writethrough similar to writethrough, a fsync is made for each write. This mode causes QEMU to interact with the disk image file or block device with both O\_SYNC and O\_DIRECT semantics, where writes are reported as completed only when the data has been committed to the storage device, and when it is also desirable to bypass the host page cache. Like writethrough cache technique, it is helpful to guests that do not send flushes when needed. It was the last cache mode added, completing the possible combinations of caching and direct access semantics.
\end{itemize}

At the RAID controller, we have similar cache techniques. The behaviors of them are same but the terminologies may be different. \textbf{Write-around} is a similar to \textbf{none} cache of QEMU, write I/O is written directly to permanent storage, bypassing the cache. This can reduce the cache being flooded with write I/O that will not subsequently be re-read, but has the disadvantage is that a read request for recently written data will create a ``cache miss`` and have to be read from slower bulk storage and experience higher latency.

OpenStack sets default cache to ``none`` for its provisioned VMs. This disables the host page cache and allow user of the guest accessing directly the storage when they open file with O\_DIRECT.

\subsection{The measurement methodology}

\subsubsection{Tools}

We relies on FIO for disk measurement in this paper. FIO is short for Flexible I/O, a versatile I/O workload generator \cite{fio}. It was written to benchmark or verify changes to the Linux IO subsystem by Linux developers. FIO is flexible enough to allow detailed workload setups, and it contains the necessary reporting to make sense of the data at completion. FIO is widely used as an industry standard benchmark, stress testing tool, and for I/O verification purposes. FIO isn't tied to any OS. It works on any platform from Windows to HP-UX to Android. There are also native IO engines on Linux, Windows, Solaris, etc. This is a key feature to allow many kinds of measurement repeatable across platforms. Besides, FIO supports three different types of output format: the default console output which dumps workload statistics at the end of the run, the file output with CSV format called Terse, and the JSON-based output file format which is far more flexible and has the advantage of being simple to parse for people and computers. However, JSON format only available in later version of FIO. We prefer the Terse format since it is available to more versions of FIO which can help us easily migrate our micro benchmark script to different platforms.

\subsubsection{Scenarios}

We run FIO on the physical host with different number of FIO threads (from 1 to 16 concurrent threads). Then with the same configuration, we run it on 5 different VM flavors (VM with one, two, four, eight, and sixteen cores). We also make the comparison with the case that each VM has only one thread but we use many concurrent active VMs on the same host to perform I/O operation. We use identical Operating Systems and identical OS' setting across platforms to make sure that the measurement result is reliable. The same fixed amount of data was used throughout the test. When there are many concurrent threads or concurrent VMs running in one test, we distribute the input data equally to all of them.

With the described scenarios above, we group our tests into three scenarios:
\begin{itemize}
  \item \textbf{Bare-metal (BM)}: multiple threads concurrently read data from or write data to the disk. Data were read from or written to a directory placed on the same hard drive.

  \item \textbf{Single VM with multiple threads (SVM-MT)}: 5 different default VM flavors of OpenStack (each flavor has different number of core) ran the same test with the involvement of 1 to 16 threads. Details on the specs of the flavors can be seen on table [?]. The first line is the name of the flavors.
      \begin{table}[h!]
      \begin{tabular}{|l|c|c|c|c|c|}
      \hline
      Resource & small & medium & large & xlarge & xxlarge \\
      \hline
      vCPU & 1 & 2 & 4 & 8 & 16 \\
      Memory (GB)& 1 & 4 & 8 & 16 & 16 \\
      Disk (GB)& 40 & 40 & 40 & 40 & 40 \\
      \hline
      \end{tabular}
      \end{table}
  \item \textbf{Multiple VMs with a single threads (MVM-ST)}: 16 low-specs VMs, each VM initiates only one process to read/write data. We would like to prevent the CPU becoming the bottle neck so we use the ``small`` flavor in this screnario with only one vCPU and 1GBs of memory.
\end{itemize}

\begin{figure*}[t]
   \centering
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/bare-metal.png}
     \caption{\textit{Bare-metal scenario}}
     \label{fig:bm}
   \end{subfigure}%
   ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
     %(or a blank line to force the subfigure onto a new line)
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/SVM-MT.png}
     \caption{\textit{SVM-MT scenario}}
     \label{fig:svm-mt}
   \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
      %(or a blank line to force the subfigure onto a new line)
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/MVM-ST.png}
     \caption{\textit{MVM-ST scenario}}
     \label{fig:mvm-st}
   \end{subfigure}
   \caption{Three scenario in the measurement}\label{fig:scenario}
\end{figure*}

\subsubsection{Workloads}
We define our workload via the job configuration file of FIO. Our sample configuration file was as follow:
\begin{lstlisting}
  ; sample setting of one thread
  [sequential-read] ; name of the thread
  rw=read             ; use sequential read
  size=(4GB/num_threads) ; size read/written per thread
  directory=/fiotest; directory for read/write
  ioengine=sync     ; I/O engine
  direct=1            ; disable OS buffering
  iodepth=1          ; in-flight IO requests per threads
\end{lstlisting}

Where:
\begin{itemize}
  \item \textbf{rw} defines the kind of disk access pattern: read (sequential reads), write (sequential writes), randread (random reads), randwrite (random writes), or mixed. We focus only on the two basic access patterns which happen mostly with Big Data workload: sequential read and sequential write. A typical example is the Hadoop File System (HDFS) which writes data in large contiguous chunks on disk, which in turn means data is written and read in large sequential operations. For HDFS, this minimizes drive seek operations - one of the slowest operations a mechanical disk can perform - and results in better performance when streaming data to workers (locally or remotely) \cite[Chapter~2]{HadoopOperationsBook}. 
 \item \textbf{size} is the total size of input data for this job. FIO will run until this many bytes of data has been transferred. We choose the total transfer volume to be 4 GBs. When doing read operator, FIO firstly writes the data to the disk. If the data size is too small, it will be cached at the OS buffer and hardware buffer. The subsequence read operator then will read from the cache but not actually read from the disk. This data size helps us to avoid many kinds of buffering from the application layer to the OS layer and then to the hardware layer. The total amount of data will be distributed equally to all the parallel threads that join the test.
  \item \textbf{direct I/O} set the value of O\_DIRECT when opening the file to read or write. I/O operations performed against files opened with O\_DIRECT bypass the kernel's page cache, writing directly to the storage. The storage may itself store the data in its cache. In our case, the storage has 512 MBs of write-back cache at the RAID controller and 64 MBs cache at the disk. Big Data application often uses the cache since the characteristics of the data is very suited to use with the cache. The data was written once and read many times. When application read the data, they do not need to modify it. So they do not care much about the integrity between the processing data and the storage data. The intermediate data which is generated during the execution will be read very soon after. Using cache is very effective for this. However, if we enable page cache, we are measure the performance of the cache, but not the storage. 
%  \item \textbf{iodepth} defines the I/O depth of the system. This defines how many IO requests to keep in flight against the file. The default is 1 for each file defined in the job description. This value can be overridden with a larger value for higher concurrency. We use I/O depth of 32 after run some initial tests to get the setting of the maximum performance of the disk. In fact, the default I/O depth in some of the Linux distributions can even be set upto 256 or 512.
  \item \textbf{ioengine} indicates the type of I/O library to use at the application. Linux supports 3 different I/O library to use: Synchronous IO (sync), Asynchronous Linux I/O (libaio), and POSIX AIO. With synchronous IO, request is sent one by one. The caller is blocked and wait for the return of the result. Native Asynchronpus IO helps to overlaps processing with I/O Operations. Application can submit batch of I/O requests without waiting for completion (required specify I/O depth which is the number of in-flight requests at a point in time). This benefits physical I/O in particular when prefetching data. Besides, it allows to accumulate read or write requests so that the I/O subsystem can optimize performance by grouping or reordering the IO requests in favor of sequential access and large request. It helps to separate calls for submission and completion indication and pipelines operations to improve throughput. It also helps to improved utilization of CPU and devices. The last library is POSIX AIO. POSIX AIOuses multiple synchronous threads from a thread pool to mimic the native AIO. QEMU uing POSIX AIO as the default when working with VM images. It also supports libaio but we have to specify in the VM configuration to enable it. We set ioengine to ``sync`` since HDFS uses Java IO class FileInputStream and FileOutputStream to serve the data to the workers or write the data to disk. All I/O is viewed as the movement of single bytes, one at a time, through an object called a Stream. These streams are synchronous. That means, that when a thread invokes a read() or write(), that thread is blocked until there is some data to read, or the data is fully written. The thread will be in blocked state for this period.
%This is a deadlock, the type of data of Big Data is option logs and others that write one but read many. Big Data application don't need to modify it, they just select the important data to cache to improve the performance. So, when we design the workload for FIO, should we also enable the cache? If we enable the cache we will only measure the memory speed, not the disk. Then if we remove the cache, what we haven't mimic the Big Data workload, so it is not representative.
\end{itemize}

The block size is 4K since big data application does not often modify the block size.

\subsection{The metrics}

During these measurements all VMs were using logical volumes created on the same physical hard disk. Physical CPU cores are not reserved for each VM, but during measurements there was enough spare capacity available to guarantee that each VM had enough cores (as configured by the VM flavor) available for its exclusive use. We run our measurement 20 times, for each run we collect the aggregated throughput of all threads and calculate the aggregated throughput. All of our figures was plotted with 95\% of confident interval.
% Increase to 20 times later, remember to fix.

\paragraph{Aggregated Throughput}
  We considered the aggregated throughput is a good metrics to compare the scenarios. %explain more

\paragraph{Jain Fairness Index}
  To evaluate fairness, we rely on the Jain Fairness Index \cite{jain98}, which is defined as
  $$JFI={({\sum}_{i}x_{i})^{2}\over n{\sum}_{i}x_{i}^{2}}$$
  where $x_{i}$ is the average throughput of the $i^th$ flow and $n$ the number of parallel threads. It features interesting properties when one wants to assess the fairness of a system. If all samples (throughput values in our case) are equal, the Jain Fairness index is simply 1, which is its maximal possible value. Also, the minimal value of the JFI is $\frac{1}{n}$. JFI is very important because its can cause stragglers in big data application. Straggler is created when a task does not receive the same amount of data as other tasks, it cannot perform its task and let the whole execution process to wait for its result. This increase the completion time of the whole job.

\paragraph{CPU utilization}

We collect CPU utilization information from two perspective: at the VM perspective and at the host perspective. In particular, at the VM, we rely on the output information extracted from FIO result. At the host, we dump the CPU utilization for our test instance every second with ``top`` command.

%FIO relies on getrusage(). Getrusage() returns resource usage statistics for the calling thread. The resource usages are returned in the structure pointed by the usage caller, which has the following form:

%\begin{lstlisting}
%struct rusage {
%    struct timeval ru_utime; // user CPU time used
%    struct timeval ru_stime; // system CPU time used
%    ...
%    long   ru_minflt;  // page reclaims (soft page faults)
%    long   ru_majflt;  // page faults (hard page faults)
%    long   ru_nswap;   // swaps
%    ...
%    long   ru_nvcsw;   // voluntary context switches
%    long   ru_nivcsw;  // involuntary context switches
%};
%\end{lstlisting}

%and contained inside the struct are:

%\begin{itemize}
%  \item \textit{ru\_utime}: This is the total amount of time spent executing in user mode, expressed in a timeval structure (seconds plus microseconds).
%  \item \textit{ru\_stime}: This is the total amount of time spent executing in kernel mode, expressed in a timeval structure (seconds plus microseconds).
%  \item \textit{ru\_nvcsw}: The number of times a context switch resulted due to a process voluntarily giving up the processor before its time slice was completed (usually to await availability of a resource).
%  \item \textit{ru\_nivcsw}: The number of times a context switch resulted due to a higher priority process becoming runnable or because the current process exceeded its time slice.
%  \item \textit{run\_time}: The duration from the time that thread starts until the time that thread finishes or the wall lock time.
%\end{itemize}

%FIO reports the CPU utilization for each thread by $\frac{ru\_utime}{run\_time}$ and $\frac{ru\_stime}{run\_time}$. However, we are more interested in the serving time (the time that thread actually uses the CPU) and the waiting time (the time that thread has to wait for the others until its slice is up).
%\begin{lstlisting}
%    serving_time = ru_utime + ru_stime
%    waiting_time = run_time - serving_time
%\end{lstlisting}

\paragraph{Disk utilization}
For disk utilization, we also collect from 2 sources. At the VM, we used the extracted information from FIO output. At the host, we use ``iostat`` to dump the disk utilization every second. Our VMs operate on a file or a virtual hard drive. We think that the storage utilisation that the VMs see may be different from the actual storage utilisation. ``iostat`` report the disk utilisation by the percentage of CPU time during the IO request was issues.

\paragraph{Latencies}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

\subsection{Aggregated Throughput}

\begin{figure*}[t]
   \centering
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/throughput_cfq_read.pdf}
     \caption{CFQ}
     \label{fig:aggthroughput_cfq_read}
   \end{subfigure}%
   ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
     %(or a blank line to force the subfigure onto a new line)
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/throughput_deadline_read.pdf}
     \caption{Deadline}
     \label{fig:aggthroughput_dealine_read}
   \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
      %(or a blank line to force the subfigure onto a new line)
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/throughput_noop_read.pdf}
     \caption{Noop}
     \label{fig:aggthroughput_noop_read}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/throughput_cfq_write.pdf}
     \caption{CFQ}
     \label{fig:aggthroughput_cfq_write}
   \end{subfigure}%
   ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
     %(or a blank line to force the subfigure onto a new line)
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/throughput_deadline_write.pdf}
     \caption{Deadline}
     \label{fig:aggthroughput_deadline_write}
   \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
      %(or a blank line to force the subfigure onto a new line)
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/throughput_noop_write.pdf}
     \caption{Noop}
     \label{fig:aggthroughput_noop_write}
   \end{subfigure}
   \caption{Aggregated throughput with the three I/O schedulers}\label{fig:aggthroughput}
\end{figure*}

\paragraph{Sequential Read}
We presented in figure \ref{fig:aggthroughput} the average aggregated throughput of our systems with three I/O schedulers on the host: CFQ, Noop, and Deadline respectively. For the 3 figures, we see two different behaviors of the virtualized platforms and the baremetal one. On one hand, the baremetal have very good performance with the CFQ scheduler, but it is not the same for the performance of the virtualized one. In all of the screnarios, the virtualized platform only gets about 40\% of aggregated throughput when number of parallel thread is increased. On the other hand, bare-metal system does not perform well with the Noop and Deadline scheduler. It is downgraded to 35\% at 4 parallel threads and to 19\% at more than 8 parallel threads. The virtualized platform sees a discrepancy at 4 parallels threads between different flavors. Flavors with 4 and 8 virtual cores can keep the aggregated throughput stable at 80MB/s with the increment of concurrency level. Flavors with 1 and 2 cores get low performance at 28MB/s, but still higher than the bare-metal one. The flavor of 16 cores was expected to have the greatest throughput, but it does not. Its performance is in the middle of the one with 4 or 8 cores and the one with 1 or 2 core. The case of MVM-ST is very weird when it gets low throughput at 17MB/s with 2 parallel threads but recover after the number of parallel threads increases.

\paragraph{Sequential Write}
For sequential write, the performance of bare-metal does not change between the host schedulers. Yet the performance of virtualized platform is a little better around 85 MB/s for Deadline and Noop in compare with the aggregated throughput at around 70 MB/s when with CFQ scheduler.
For this access pattern, we see that the performance is affected mostly by write-back cache and write-through cache of the the RAID controller device.

We presented the IO performance of sequential write when we using write-through cache in figure \ref{fig:throughputwritethrough}.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.7]{figures/throughputwritethrough.pdf}
  \caption{\textit{Write-through cache is using on the host and the VMs}}
  \label{fig:throughputwritethrough}
\end{figure}

\paragraph{Discussion}
To explain for the result that we got, we have ask ourselves many questions:
\begin{itemize}
 \item \textit{In case of using Deadline and Noop scheduler, why there is different behavior for different VM flavors of SVM-MT scenario?}
 \item \textit{We see that IO performance is changed when VMs have different number of cores. Do reducing the number of online (not-disable) cores on bare-metal will reduces IO performance of bare-metal?} we reduced the number of online cores on bare-metal to see if IO performance of bare-metal is affected. However, for all the different numbers of online cores on the bare-metal, we still get similar throughput. (Figure \ref{fig:throughputbmdiffonlinecore})
     \begin{figure*}[t]
      \centering
      \includegraphics[scale=0.7]{figures/throughput_cfq_bmdiffonlinecore.pdf}
      \caption{\textit{Throughput of baremetal with different number of online cores}}
      \label{fig:throughputbmdiffonlinecore}
     \end{figure*}
 \item \textit{Why does the deadline scheduler and Noop scheduler has similar result?} The Deadline elevator uses a deadline algorithm to minimize I/O latency for I/O requests. A standard elevator algorithm is used, unless the request at the head of one of the FIFO queues grows older than an expiration value. In that case the scheduler begins servicing expired requests. The results shown in this study show the Deadline scheduler behaving almost identically to the Noop scheduler. It seems likely that the deadlines imposed by the scheduler are never exceeded, resulting in the Deadline scheduler devolving to the same behavior as the Noop scheduler.
 \item \textit{Why does throughput reduce from 2 concurrent threads?} With one thread, the seek time is optimals. From 2 concurrent threads, seeking will happen and affect on the throughput.
 \item \textit{What exactly is happening at the hypervisor level in the scenario with MVM-ST? Why do we have an increasing trend?} For the host itself, a VM instance is just a process. In VM, we use O\_DIRECT flag to bypass the guest's kernel page caches. But at the host, Page cache still happen. When we increase the nulber
 \item \textit{Why is READ performance worse than WRITE?} For write performance, we are writing to the cache of the RAID controller, but not actually writing to the disk. It is clearly showed in the figure with the cache-through technique (figure \ref{fig:throughputwritethrough}), when the OS waits for system calls return after the I/O request was written successfully to disk, we have less output.
 \item \textit{Is the disk or the vdisk the bottleneck?} % To answer this, we need to find the disk utilization from the perspectives of both the guest OS and host OS.
 \item \textit{Is the CPU or vCPU the bottleneck?} % To answer this, we need to find the CPU utilization from the perspectives of both the guest OS and host OS.
 \item \textit{Why does SMV-MT scenario got low throughput when using CFQ scheduler at the host?} CFQ scheduler maintains 64 per-process queues for synchronous IO request while it only maintains one queue for asynchronous IO request. We checked the fairness of two platforms when using CFQ and saw that BareMetal linearly reduced its fairness from 1 (at 1 thread) to 0.85 (at 16 threads) while the virtualized platform maintains its perfect fairness at 1 during the experiments (as in figure \ref{fig:jfi}). We guess that eventhough we setup FIO to use asynchronous I/O in the guest OS, but after the IO requests left the guest OS, it was treated as synchronous IO at the host domain. CFQ get perfect fairness because it serves each IO queue of each process with an equal quantum time, when using only one queue for all the processes, the system would lose its fairness. Synchronous IO has lower performance since it is a blocking mechanism.
\end{itemize}


\paragraph{CPU utilization}


\paragraph{Context switching}
\begin{figure*}[t]
   \centering
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/contextswitching_cfq_read.pdf}
     \caption{CFQ}
     \label{fig:contextswitching_cfq_read}
   \end{subfigure}%
   ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
     %(or a blank line to force the subfigure onto a new line)
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/contextswitching_deadline_read.pdf}
     \caption{Deadline}
     \label{fig:contextswitching_dealine_read}
   \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
      %(or a blank line to force the subfigure onto a new line)
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/contextswitching_noop_read.pdf}
     \caption{Noop}
     \label{fig:contextswitching_noop_read}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/contextswitching_cfq_write.pdf}
     \caption{CFQ}
     \label{fig:contextswitching_cfq_write}
   \end{subfigure}%
   ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
     %(or a blank line to force the subfigure onto a new line)
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/contextswitching_deadline_write.pdf}
     \caption{Deadline}
     \label{fig:contextswitching_deadline_write}
   \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
      %(or a blank line to force the subfigure onto a new line)
   \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{figures/contextswitching_noop_write.pdf}
     \caption{Noop}
     \label{fig:contextswitching_noop_write}
   \end{subfigure}
   \caption{Number of context switchings with the three I/O schedulers}\label{fig:contextswitching}
\end{figure*}

Context Switching (as seen in figure \ref{fig:contextswitching}) is not affected by the I0 schedulers, but looking at coherent between the Aggregated throughput and the number context switch, we see that the more context switching happen, the lower throughput a VM can have. VM with 1 or 2 cores switch 2 times more than VM with 4 and 8 cores. We also see that increasing number of cores to 16 does not reduce the number of context switchings.

\subsection{Jain Fairness Index}
In our experiments, for all the I/O schedulers and platforms. Fairness index is very high. Fairness may not show much the different between bare-metal and virtualization in term of performance, but why did we select it as a metric for the comparison? Because fairness has big impacts on Big Data application. Whenever a task is not received the same amount of data as other tasks, it will become a straggler \cite{matei08}. In the context of big data application, a straggler is very bad. 1) For Hadoop and Spark, in particular, the slowest task determines the overall job completion time. Straggler causes the whole execution to wait for some slow tasks before it can finish and return the results. 2) Many systems like Hadoop and Spark deploy task speculation. This is an optimization technique to performs some tasks that may not be actually needed. The main idea is to run a task before it is known whether the results of that task will be needed, so as to prevent a delay incurred by re-run that task after it is known whether it is needed. If it turns out the work was not needed after all, any result produced by the speculative tasks is ignored. If we have a fair system, we can avoid wasting resources for the speculative tasks.

\begin{figure*}[t]
  \includegraphics[scale=0.85]{figures/JFI.pdf}
  \caption{\textit{Jain Fairness Index of different scenarios with CFQ scheduler}}
  \label{fig:jfi}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
By using a synthesis workload mimic from a MapReduce-like application, we reveal the overhead of virtualization in term of IO performance. We confirm the results of \cite{khoa13,boutcher10} and giving further details on the I/O performance of the virtualized and baremetal systems respected to level of concurrency and Big-data-like workload. We have found and confirmed that:
\begin{itemize}
  \item The schedulers of the host play an important role in the behavior of the performance of the system. A cloud administrator should choose Deadline of Noop scheduler for there Big Data application. If they want to use the baremetal one, CFQ is a better choice since it works well with data intensive workload. The virtualization platform achieve better aggregated throughput with Deadline and Noop scheduler. In addition, It's depended on the number of cores to achieve the greatest throughput for one VM instance in the cloud. Currently we do not have a method to select the best specs for the VM with a given host specs in favor of I/O performance. But the reader can repeat our experiment with our automated scripts \ref{link:github} to select the best flavor for their hosts.
  \item Both platform achieve great fairness index in term of IO performance. Keeping our eyes on the fairness index enables us to explain the bad performance of SVM-MT with the CFQ scheduler.
  \item For sequential write workload, we showed that the virtualized platform lose 15-30\% in compare with the bare-metal. Both platform are affected greatly by the caching technique that deploy on the RAID controller. In details, Write-back and Write-through cache play important role in the performance of sequential writing for both the bare-metal and the virtualized platform. For Hadoop-like workload, user should use write-back cache since it's perform well with data intensive workload.
\end{itemize}

We believe that this work gives deeper understanding on the impacts of virtualization Big Data application workload. However, this is just the first part in a continuation research which studies on the impacts of virtualization on Data Intensive Analysis Framework in which there are two most popular representatives: Hadoop and Spark. Those application performance can be bounded by network, disk I/O, or CPU performance. In this paper, we focus on and only on disk I/O and it is the limitation of our work. This work is still limited in explaining the insides that cause the seperated performance between VM flavors. We also avoid memory and CPU over-commit which is happen often happen when using virtualization technology. Performance of Big Data application is not only depended on storage I/O performance, but if one component is better, the whole process should be better. In future, we would like to see how the I/O performance was seen from the perspective of the big data application (such as Hadoop or Spark). From then we can prove the correctness of our synthesis workload design.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%
% THIS PART IS FOR THE REFERENCE SECTION
%%%%%%%%%%%%
\balance
\bibliographystyle{abbrv}
\bibliography{ref}

\end{document}
