\documentclass{acmsig}

\usepackage[color=yellow,obeyFinal]{todonotes}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{float}
\usepackage{varioref}
\usepackage{mathtools}
\usepackage{xspace}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{balance}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{paralist}
\usepackage{cite}
\usepackage{color}

\usepackage{xcolor}
\definecolor{dark-red}{rgb}{0.4,0.15,0.15}
\definecolor{dark-blue}{rgb}{0.15,0.15,0.4}
\definecolor{medium-blue}{rgb}{0,0,0.5}
\hypersetup{
  colorlinks, linkcolor={dark-red},
  citecolor={dark-blue}, urlcolor={medium-blue}
}

\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\etc}{\textit{etc.}\xspace}
\newcommand{\vs}{\textit{vs.}\xspace}
\newcommand{\keyval}{$\langle\text{key}, \text{value}\rangle$\xspace}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\newcommand{\noteby}[2]{\todo[inline]{#2\hspace*{\fill}\mbox{ --#1}}}

\lstset{frame=tb,
  language=SQL,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{blue},
  stringstyle=\color{mauve},
  keywordstyle=\color{blue},
  commentstyle=\color{Brown},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

% paper title
% can use linebreaks \\ within to get better formatting as desired
% \title{Automatic MapReduce ROLLUP}
\title{This is our title}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\numberofauthors{4}
\author{
\alignauthor
Ha Son Hai\\
       \affaddr{Orange/EURECOM}\\
       \email{sonhai@eurecom.fr}
\alignauthor
Daniele Venzano\\
       \affaddr{EURECOM}\\
       \email{venzano@eurecom.fr}
\and
\alignauthor
Patrick Brown
       \affaddr{Orange}\\
       \email{p.brown@orange.fr}
\alignauthor
Pietro Michiardi
       \affaddr{EURECOM}\\
       \email{michiard@eurecom.fr}
}

% make the title area
\maketitle


\begin{abstract}
Still very abstract
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
  \item brief introduction virtualization (2-3 sentences)
  \item brief introduction data intensive processing framework (2-3 sentences)
  \item one sentence for problem statement
  \item two sentences for related work
  \item two sentences for methodology
  \item two sentences for results and assessment
  \item one sentences for conclusion of introduction
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and Related Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
  \item introduce some related works on the performance of virtualization: explaining packet delay under virtualization, works from Guillaume and Dino,
  \item introduce some related works on measuring performance of data intensive application
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem statement}
In the Big Data world, data volume is too big that people avoiding moving it. With the belief that network is the bottleneck of the distributed application, the distributed system (like Hadoop with MapReduce framework) developers try to work around this by moving the execution/programs to the workers but not the data. By this method of task distribution, storage I/O (disk and memory) was fully utilised to get the most out of the processing system. In the mean time, memory and cache sharing was not developed well enough to provide fault tolerant to the distributed system. That is why the data intensive applications greatly depend on the hard drive performance.

The trending now is moving to the cloud, where people can rent a part of the virtualized infrastructure to process there data. (introduce little about Iaas, PaaS, and SaaS).
Trending in private cloud. All of these brings the virtualization technology on top of choices for flexiple platform. (I also don't know what i mean here). 

When running Hadoop/ data intensive application in virtualized environment, the performance of the IO now is very important. Everything depend on disk IO. Temporary data was stored on disk. Data was read from disk. Results was written to this. But the number of study (on this) are counting with just fingers. (Then have to list these studies). In our study, we see that Virtual Machine can only reach {n}\% of what the disk can give, even in quiet period.

Also need to talk about network I/O (because we have it in the later part, make it removable, so if we do not have the result for network test, we can omit it easily)

There is a common belief that virtualization
We wonder if it is really true. And if it is bad, how bad it is? Is there any methodology that can help us to gain more performance with virtualization? In this report we present our study on the performance of I/O operation in virtualized environment. We make the experiments with the three layers: bare-metal (or the performance of host), guest operating system, and application.
When we can measure the overhead of the virtualization, we can make better decision on
\begin{itemize}
  \item \textit{What we do?}
  \begin{itemize}
    \item Experimental study of virtualization overheads for I/O operations
    \item We want to have the typical I/O ``pressure'' that comes from data-intensive applications
    \item We want to have the view from the ``guest operating system'' and the view from the applications (and see if there are differences)
  \end{itemize}
  \item \textit{Why and why is it important?}
  \begin{itemize}
    \item It is common wisdom that virtualization is ``bad'' for performance. Is it true?
    \item Given the knowledge about performance bottlenecks and eventual overheads of virtualization, is this going to be helpful for the applications we consider?
  \end{itemize}
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The methodology}

\subsection{The system under measurement}
\begin{itemize}
  \item Platform description
  \begin{itemize}
    \item Hardware
    \begin{itemize}
      \item Network architecture
      \item Disk subsystem: RAID controller, ...
    \end{itemize}
    \item Software
    \begin{itemize}
      \item Hypervisor
      \item OVS
      \item Host file-system
      \item LVM
    \end{itemize}
  \end{itemize}
\end{itemize}
\noteby{dv}{I'm skipping the host file-system description, since it does not matter in this context, measurements were done over plain LVM volumes}

\subsubsection{Hardware}
Our cluster uses a heterogeneous set of physical machines: we have two \textit{master} nodes running on a dual quad-core Xeon L5320 server clocked at 1.86GHz, with 16GB of RAM, two 1TB hardware RAID5 volumes, and two 1Gbps network interfaces; \textit{worker} nodes execute on six dual exa-core Xeon E5-2650L (with hyperthreading enabled) servers clocked at 1.8GHz, with 128GB of RAM, ten 1TB disks (configured as JBOD, Just a Bunch of Disks).

\paragraph{Network}
Hypervisors are connected via three 1Gbit/s bonded interfaces on each physical host. OpenVswitch uses these bonded interfaces to establish GRE tunnels and connect the virtual switches on each host.
Hosts are interconnected to top-of-rack switches with sufficient backplane capacity to handle all traffic generated by the hosts.

\paragraph{Disk}
The hardware RAID provided by the server hardware has been disabled in favor of a JBOD configuration. This exposes to the operating system many more options to configure and measure the disk subsystem. Disks are all the same make and model, across the hosts: SEAGATE ST91000640SS.

\subsubsection{Software}

\paragraph{Hypervisor}
Each machine in the cluster runs the same Linux distribution, a Ubuntu 12.04 LTS, updated with the most recent patches. All energy saving settings in the BIOS are disabled, since they cause severe performance penalties. Bigfoot uses the KVM hypervisor, with \texttt{virtio} and \texttt{vhost\_net} acceleration modules enabled. Virtualization support in the CPUs is enabled (VMX) and KVM uses it automatically. The hypervisor is configured by Nova to use LVM for VM storage.
\noteby{dv}{Son Hai: describe the VM flavor and image used}

\paragraph{OpenVSwitch (OVS)}
On the Bigfoot platform, the most common OpenStack setup has been implemented. The network component, \texttt{Neutron}, is configured to use the OpenVSwitch plugin to provide connectivity between VMs. OVS is a software switch implementation that materializes as a virtual switch spanning across multiple physical hosts. In our configuration, \texttt{Neutron} creates a single OVS switch for all VMs, on each Hypervisor, using VLAN tagging to separate traffic from different tenants.
To provide connectivity between tenants and the external network, the virtual network is configured according to the \emph{provider router with private networks} use-case described in the OpenStack documentation.\noteby{dv}{Citation needed}
Thus, each tenant has its own IP subnet, and exchange traffic between each other and the Internet using a single virtual router connected to the subnets of each tenant from one side and to the external network on the other side. The \texttt{Neutron} virtual router is implemented as network namespace on the master node, where a number of NAT and routing rules provide interconnection, external access and floating IPs allocated to the VMs.

\paragraph{Logical Volume Manager (LVM)}
The storage backend for KVM is LVM. On each hypervisor, a volume group consisting of seven 1TB disks is used by Nova to dynamically create logical volumes for the ephemeral disks of KVM instances.

\subsection{The measurement methodology}
\begin{itemize}
  \item A software framework to perform repeatable experiments
  \item A way to collect ``logs'' and analyze them
  \item A way to describe ``application-level'' I/O patterns, and implement them through measurements
  \item A way to instrument or to use the logs of data-intensive applications to collect measurement from their perspective
  \begin{itemize}
    \item Applications we consider: Hadoop, Spark, NoDB
  \end{itemize}
\end{itemize}

\subsection{The performance/overheads metrics}
\noteby{pm}{Should we consider over subscription as an important parameter??????}
\begin{itemize}
  \item Statistical methodology
  \item Metrics:
  \begin{itemize}
    \item Network
    \begin{itemize}
      \item Throughput
      \item Latency
      \item Jain Fairness index (in case of concurrent access patterns)
      \item CPU vs. network utilization
    \end{itemize}
    \item Disk
    \begin{itemize}
      \item Throughput
      \item Latency
      \item Jain Fairness index (in case of concurrent access patterns)
      \item CPU vs. disk utilization
    \end{itemize}
  \end{itemize}
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

\begin{itemize}
  \item We present results per each scenario we consider, such that here we will have a number of subsections that are ``self-contained'', that is, they describe the scenario, to which application pattern it is matching, and then we present the results
\end{itemize}

During these measurements all VMs were using logical volumes created on the same physical hard disk. Physical CPU cores are not reserved for each VM, but during measurements there was enough spare capacity available to guarantee that each VM had a single core (as configured by the VM flavor) available for its exclusive use.

\subsection{Single physical host}
\noteby{pm}{what does exactly mean ``fat''? should we make it with capacity equal to the sum of the capacity of multiple VMs?}
\noteby{dv}{We should take into account, for the measurements that include interference, which CPU Package the interfering vm is placed in}
\begin{itemize}
  \item Describe the ``scenario'' for this case (as a side note, we could also say we look at ``normal'' databases)
  \item Use the metrics above to compare bare-metal vs. virtual: this is essentially the overhead we want to study
  \item Disk
  \begin{itemize}
    \item Single (fat) VM
    \begin{itemize}
      \item Concurrency: from 1 to many parallel/concurrent I/O requests, using threads
      \item Interference: e.g. have a disturbing VM that uses a lot of CPU. Note that this makes much more sense if the host is ``oversubscribed''
    \end{itemize}
    \item Multiple VMs
    \begin{itemize}
      \item Concurrency: from 1 to many parallel/concurrent I/O requests (equally distributed among VM)
      \item Interference: e.g. have a disturbing VM that uses a lot of CPU. Note that this makes much more sense if the host is ``oversubscribed''
    \end{itemize}
  \end{itemize}
  \item Network
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%
% THIS PART IS FOR THE REFERENCE SECTION
%%%%%%%%%%%%
% \balance
% \bibliographystyle{abbrv}
% \bibliography{ref}

\end{document}
